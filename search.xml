<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>leetcode-二分查找</title>
      <link href="/2025/05/02/leetcode-%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/"/>
      <url>/2025/05/02/leetcode-%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/</url>
      
        <content type="html"><![CDATA[<h1 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a>二分查找</h1><p>二分查找的主要问题在于while循环时left与right的&lt;&#x3D;以及&lt;之间的判断条件。这里先给出结论</p><p><strong>左闭右闭：left&lt;&#x3D;right，例：[1, 1]</strong></p><p><strong>左闭右开：left&lt;right, 例：[1, 1)</strong></p><p>主要问题在于右开部分，当左闭右闭时，我们需要完整遍历整个数组，即最右边的那个边界数我们也需要遍历到进行判断。但是当左闭右开时就不需要遍历最后的这个边界数了，因此左闭右开时<code>int right = nums.size();</code>同理<code>right = mid</code>情况也是，需要去除最外界这个数，因为右边界的这个数实际是不需要遍历的。</p><p>例子：[1, 3, 5, 7, 9]</p><p>左闭右闭时，我们的索引为(0, 1, 2, 3, 4)，因此我们需要遍历的范围为0-5都要包含在内；而左闭右开时，我们需要遍历的区间则为0-5，即<code>while(left&lt;right)</code>时，因为右区间为开，整个循环会比左闭右闭时多循环一次达到一个溢出范围。其实对比起来就是左闭右闭时区间为[0,  4]，左闭右开时为[0, 5)，因为实际并不会遍历到5这个索引号，即不存在左等于右的情况。同理缩小查找范围也是，不需要考虑nums[5]这个情况，因为会溢出。其实就是在左闭右闭的情况上加了一位，</p><h3 id="左闭右闭"><a href="#左闭右闭" class="headerlink" title="左闭右闭"></a>左闭右闭</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">class Solution &#123;</span><br><span class="line">public:</span><br><span class="line">    int search(vector&lt;int&gt;&amp; nums, int target) &#123;</span><br><span class="line">        // 左闭右闭</span><br><span class="line">        int left = 0;</span><br><span class="line">        int right = nums.size() - 1;    // 数组索引下标从0开始，一直到最后一个索引号。其实这里就代表了右边界的范围</span><br><span class="line"></span><br><span class="line">        while(left &lt;= right) &#123;  // 会遍历到最后left=right有意义</span><br><span class="line">            int mid = (left + right) / 2;</span><br><span class="line">            if(nums[mid] &lt; target)    // 当前数比目标小，那么我们需要去右边更大的范围里找</span><br><span class="line">                left = mid + 1;</span><br><span class="line">            else if(nums[mid] &gt; target)</span><br><span class="line">                right = mid - 1;</span><br><span class="line">            else</span><br><span class="line">                return mid;</span><br><span class="line">        &#125;</span><br><span class="line">        return -1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="左闭右开"><a href="#左闭右开" class="headerlink" title="左闭右开"></a>左闭右开</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">class Solution &#123;</span><br><span class="line">public:</span><br><span class="line">    int search(vector&lt;int&gt;&amp; nums, int target) &#123;</span><br><span class="line">        // 左闭右开</span><br><span class="line">        int left = 0;</span><br><span class="line">        int right = nums.size() ;// 左闭右开时比左闭右闭时多了一位，因此就是nums.size() - 1 + 1 = nums.size()</span><br><span class="line"></span><br><span class="line">        while(left &lt; right) &#123;</span><br><span class="line">            int mid = (left + right) / 2;</span><br><span class="line">            if(nums[mid] &lt; target)</span><br><span class="line">                left = mid + 1;</span><br><span class="line">            else if(nums[mid] &gt; target)</span><br><span class="line">                right = mid;</span><br><span class="line">            else</span><br><span class="line">                return mid;</span><br><span class="line">        &#125;</span><br><span class="line">        return -1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>日语学习</title>
      <link href="/2025/04/06/%E6%97%A5%E8%AF%AD%E5%AD%A6%E4%B9%A0/"/>
      <url>/2025/04/06/%E6%97%A5%E8%AF%AD%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="日语学习计划"><a href="#日语学习计划" class="headerlink" title="日语学习计划"></a>日语学习计划</h1><h2 id="1-五十音"><a href="#1-五十音" class="headerlink" title="1. 五十音"></a>1. 五十音</h2><h2 id="2-初步学习（1个月）"><a href="#2-初步学习（1个月）" class="headerlink" title="2. 初步学习（1个月）"></a>2. 初步学习（1个月）</h2><p>以《大家的日语》为例，初级上下两本，1.5倍速迅速搭建框架，保证有一个大体的印象，可以写一个思维导图框架辅助。语法这里只需要搭建框架，后面以刷蓝宝书巩固为主。</p><p>主要以单词背诵为主，制定一个学习单词的计划：</p><p>每天50-200词汇，然后第二天复习第一天的词汇（这个过程会很痛苦但是请坚持这个过程）</p><p><strong>单词是最为重要的务必反复记忆！！！</strong></p><p>后面的课后习题和单词，如果你用APP背诵就不用看单词了，可以适当的写写课后习题不过，其实可以拿新完全掌握练练手。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>vscode-ssh使用</title>
      <link href="/2025/04/02/vscode-ssh%E4%BD%BF%E7%94%A8/"/>
      <url>/2025/04/02/vscode-ssh%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="1-通过VSCODE配置SSH进行远程服务器连接"><a href="#1-通过VSCODE配置SSH进行远程服务器连接" class="headerlink" title="1. 通过VSCODE配置SSH进行远程服务器连接"></a>1. 通过VSCODE配置SSH进行远程服务器连接</h2><p>打开VSCODE，在左边的状态栏找到扩展项，然后再扩展项搜索栏输入remote-ssh找到该插件选择安装。</p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202504021641193.png" alt="image-20250402164138041"></p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202504021643344.png" alt="image-20250402164325246"></p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202504021644901.png" alt="image-20250402164408812"></p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202504021709869.png" alt="image-20250402170931781"></p><p>安装完成后，在左边的状态栏找到远程资源管理器并进入。点击SSH旁边的+号添加服务器，在正上方的状态栏内输入服务器SSH登录指令并回车（ssh -p 32467 <a href="mailto:&#x72;&#x6f;&#x6f;&#116;&#x40;&#x63;&#x6f;&#110;&#110;&#x65;&#x63;&#x74;&#46;&#110;&#109;&#x62;&#49;&#46;&#115;&#101;&#x65;&#116;&#97;&#99;&#108;&#111;&#117;&#100;&#x2e;&#x63;&#x6f;&#109;">&#x72;&#x6f;&#x6f;&#116;&#x40;&#x63;&#x6f;&#110;&#110;&#x65;&#x63;&#x74;&#46;&#110;&#109;&#x62;&#49;&#46;&#115;&#101;&#x65;&#116;&#97;&#99;&#108;&#111;&#117;&#100;&#x2e;&#x63;&#x6f;&#109;</a>），随后在该处选择第一项.ssh文件路径下的config配置文件生成</p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202504021713576.png" alt="image-20250402171300543"></p><p>config文件内容如下图所示。</p><p><strong>Host</strong>：用户定义的别名，可以随便起一个，就是上图绿色标志旁边显示的名称</p><p><strong>HostName</strong>：服务器的ip地址，不可随意更改</p><p><strong>Port</strong>：SSH连接的服务器端口号</p><p><strong>User</strong>：连接到服务器时的用户名，这里是管理员账户“root”</p><p><strong>IdentityFile</strong>：免密登录的ssh密钥保存位置，通过配置ssh公钥和密钥可以免去每次登录时输入密码的麻烦（<strong>需要提前安装git，在下面会说明</strong>）</p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202504021714034.png" alt="image-20250402171421000"></p><p>在编辑完config文件后，可以看到在SSH下面出现刚刚配置的服务器，我们点击红框内的任意按钮进行连接即可。</p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202504021721933.png" alt="image-20250402172106891"></p><p>此时可能会出现平台选择，这里我们选择“Linux”，然后按照情况输入密码进行连接即可。</p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202504021723989.png" alt="image-20250402172322958"></p><h2 id="2-配置git进行免密连接"><a href="#2-配置git进行免密连接" class="headerlink" title="2. 配置git进行免密连接"></a>2. 配置git进行免密连接</h2><h4 id="1-通过命令ssh-keygen生成新的密钥对"><a href="#1-通过命令ssh-keygen生成新的密钥对" class="headerlink" title="1. 通过命令ssh-keygen生成新的密钥对"></a>1. 通过命令<code>ssh-keygen</code>生成新的密钥对</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 在 Linux 和 Mac 上</span><br><span class="line">ssh-keygen -t rsa -b 2048 -f ~/.ssh/id_rsa</span><br><span class="line">​</span><br><span class="line"># 在 Windows 上</span><br><span class="line">ssh-keygen -t rsa -b 2048 -f C:\Users\YourUsername\.ssh\id_rsa</span><br></pre></td></tr></table></figure><p><strong>注意：</strong>当你在多个平台上使用 SSH 连接到不同的远程服务器时，可能需要为每个平台生成和使用不同的密钥对。这是因为每个平台（例如，Windows、Linux、Mac）可能有不同的文件系统和密钥文件位置，同时在安全性的考虑下，不同平台上的密钥对最好是独立的。</p><p>输入命令后一路回车即可，系统会在指定路径下生成两个文件，分别是<code>id_rsa.pub</code>和<code>id_rsa</code>，前者为生成的公钥，后者为私钥 。</p><h4 id="2-将公钥添加到远程服务器"><a href="#2-将公钥添加到远程服务器" class="headerlink" title="2. 将公钥添加到远程服务器"></a>2. 将公钥添加到远程服务器</h4><p>将生成的公钥（id_rsa.pub）复制添加到远程服务器的<code>authorized_keys</code>文件中，如下图所示（添加新的请不要覆盖前面的，空一行后在下面添加新的）</p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202504021738126.png" alt="image-20250402173805089"></p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202504021738466.png" alt="image-20250402173835430"></p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202504021736937.png" alt="image-20250402173612878"></p><h3 id="3-添加路径配置到SSH本地配置文件中"><a href="#3-添加路径配置到SSH本地配置文件中" class="headerlink" title="3. 添加路径配置到SSH本地配置文件中"></a>3. 添加路径配置到SSH本地配置文件中</h3><p>打开VSCODE，选择红框的小齿轮打开配置文件。</p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202504021740707.png" alt="image-20250402174019672"></p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202504021740617.png" alt="image-20250402174049584"></p><p>新起一行（注意缩进）添加IdentityFile秘钥<code>id_rsa</code>的路径后保存。</p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202504021741583.png" alt="image-20250402174111552"></p><p>当完成上述配置后，每次连接ssh服务器时，就不需要每次输入密码，实现免密登录。</p><p>TIPS:</p><p><strong>登录指令</strong>：ssh -p 32467 <a href="mailto:&#x72;&#111;&#111;&#x74;&#x40;&#x63;&#x6f;&#110;&#110;&#x65;&#99;&#116;&#46;&#110;&#109;&#x62;&#49;&#x2e;&#115;&#x65;&#101;&#x74;&#97;&#x63;&#x6c;&#111;&#117;&#100;&#46;&#99;&#111;&#x6d;">&#x72;&#111;&#111;&#x74;&#x40;&#x63;&#x6f;&#110;&#110;&#x65;&#99;&#116;&#46;&#110;&#109;&#x62;&#49;&#x2e;&#115;&#x65;&#101;&#x74;&#97;&#x63;&#x6c;&#111;&#117;&#100;&#46;&#99;&#111;&#x6d;</a></p><p><strong>密码</strong>：KuWrLHXNYBXN</p><p>数据尽量放在autodl-tmp中，在vscode中的终端窗口选择conda环境或者<code>crtl+shift+P</code>选择Python环境</p>]]></content>
      
      
      <categories>
          
          <category> SSH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SSH </tag>
            
            <tag> VSCODE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>特征工程</title>
      <link href="/2025/03/25/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
      <url>/2025/03/25/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h1><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>想象一下，你和朋友去看电影。电影还没结束，你朋友就说：“这电影肯定五星！”而你一脸懵：“怎么看出来的？”他说：“导演是大咖，演员演技在线，剧情节奏也好。”你朋友其实就是在“特征工程”——他从电影里挑出关键的“特征”，帮他判断电影好坏。</p><p>它就像是给模型“做饭”：从一堆杂乱的原始食材（数据）中，挑出有用的部分，洗干净、切好、调味，最后做成一道模型爱吃的“大餐”（特征）。说得简单点，特征工程就是通过数学和统计的方法，把原始数据变成模型能更好理解和学习的形式。</p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202503250049927.png" alt="image-20250325004940861"></p><p><strong>Memory-Based</strong>（基于记忆算法）：红色圆圈。</p><p><strong>Winnow</strong>：蓝色交叉线。</p><p><strong>Perceptron</strong>（感知机）：绿色三角形。</p><p><strong>Naïve Bayes</strong>（朴素贝叶斯）：蓝色方框。</p><h2 id="1-特征提取"><a href="#1-特征提取" class="headerlink" title="1. 特征提取"></a>1. 特征提取</h2><p>特征提取是从原始数据中“创造”出一组新的、更具信息量的特征的过程。简单来说，就是把杂乱的原始数据“浓缩”成更有用的东西，让模型更容易理解。</p><p><strong>通俗解释</strong>：<br>想象你在煮汤，原始数据是一堆食材（土豆、胡萝卜、肉），特征提取就像把这些食材熬成一碗浓汤，只保留精华部分。</p><p><strong>目的</strong>：</p><ul><li>把复杂或高维的数据（比如图片、文本）变成模型能处理的数值特征。</li><li>减少数据维度，降低计算复杂度。</li><li>挖掘数据的深层信息。</li></ul><p><strong>常见方法</strong>：</p><ul><li><strong>主成分分析（PCA）</strong>：把多个相关特征“压缩”成几个新特征，保留主要信息。</li><li><strong>TF-IDF</strong>：从文本中提取词的重要性，转化为数值。</li><li><strong>图像边缘检测</strong>：从图片像素中提取边缘或纹理特征。</li></ul><p><strong>例子</strong>：<br>假设你有一堆图片（原始数据是像素点），特征提取可以用算法找出图片里的边缘或颜色分布，这些新特征比原始像素更能帮模型识别内容。</p><h2 id="2-特征选择"><a href="#2-特征选择" class="headerlink" title="2. 特征选择"></a>2. 特征选择</h2><p><strong>定义</strong>：<br>特征选择是从已有特征中“挑选”出一部分最有用的子集，不改变特征本身，只是决定留下哪些、丢掉哪些。</p><p><strong>通俗解释</strong>：<br>还是煮汤的比喻，特征选择不是熬汤，而是从一堆食材里挑出最有营养的（比如肉和胡萝卜），扔掉没用的（比如烂叶子）。</p><p><strong>目的</strong>：</p><ul><li>去掉无关、冗余或噪声特征，提升模型准确性。</li><li>减少特征数量，加快训练速度。</li><li>提高模型的可解释性。</li></ul><p><strong>常见方法</strong>：</p><ul><li><strong>过滤法</strong>：用统计指标（如相关系数）挑特征。</li><li><strong>包装法</strong>：用模型测试哪个特征组合最好（比如递归特征消除）。</li><li><strong>嵌入法</strong>：在训练中自动选特征（比如LASSO回归）。</li></ul><p><strong>例子</strong>：<br>预测房价时，数据有房屋面积、房间数、房子颜色等特征。特征选择发现面积和房间数对房价影响大，颜色没啥用，就扔掉颜色。</p><h2 id="3-特征构建"><a href="#3-特征构建" class="headerlink" title="3. 特征构建"></a>3. 特征构建</h2><h3 id="1-归一化和标准化"><a href="#1-归一化和标准化" class="headerlink" title="1. 归一化和标准化"></a>1. 归一化和标准化</h3><h4 id="1-归一化（最大-最小标准化）"><a href="#1-归一化（最大-最小标准化）" class="headerlink" title="1. 归一化（最大-最小标准化）"></a>1. 归一化（最大-最小标准化）</h4><p><strong>什么是归一化？</strong><br>归一化就像把一堆大小不一的数字“挤”进一个统一的范围，通常是0到1。想象你有一堆考试成绩，有的满分是100，有的满分是50，归一化能让它们看起来都像是一个标准的分数范围，这样比较起来就公平了。</p><p><strong>数学公式：</strong><br>$x^* &#x3D; \frac{x - x_{\text{min}}}{x_{\text{max}} - x_{\text{min}}}$</p><ul><li>$x$ 是原始数据的值。</li><li>$x_{\text{min}}$ 是所有数据中最小的值。</li><li>$x_{\text{max}}$ 是所有数据中最大的值。</li><li>$x^*$ 是标准化后的新值。</li></ul><p><strong>怎么理解这个公式？</strong></p><ul><li>减去 $x_{\text{min}}$：把最小值移到0。</li><li>除以 $x_{\text{max}} - x_{\text{min}}$：把整个范围压缩到0到1之间。</li><li>结果：最小值变成0，最大值变成1，其他值按比例落在中间。</li></ul><p><strong>举个例子：</strong><br>假设你的数据是 <code>[10, 20, 30, 40, 50]</code>：</p><ul><li>$x_{\text{min}} &#x3D; 10$, $x_{\text{max}} &#x3D; 50$。</li><li>对于 $x &#x3D; 20$：<br>$x^* &#x3D; \frac{20 - 10}{50 - 10} &#x3D; \frac{10}{40} &#x3D; 0.25$</li><li>结果：<code>[0, 0.25, 0.5, 0.75, 1]</code>。</li></ul><p><strong>图片里说了什么？</strong></p><ul><li>归一化的目标是让不同特征（比如身高、体重）在同一个量纲下，避免因为数值范围差异太大而影响模型。</li><li>但它可能会改变数据的分布形状，比如原来分布很“扁平”的数据可能被“拉长”或“压缩”。</li></ul><p><strong>什么时候用？</strong><br>当你的数据范围差别很大（比如一个特征是0-1000，另一个是0-1），或者模型（像神经网络）对数据尺度敏感时，归一化很管用。</p><h4 id="2-Z-Score标准化"><a href="#2-Z-Score标准化" class="headerlink" title="2. Z-Score标准化"></a>2. Z-Score标准化</h4><p><strong>什么是Z-Score标准化？</strong><br>Z-Score标准化是把数据“拉平”，让它围绕平均值分布，看起来更“标准”。它不会把数据硬塞进0到1，而是让数据的平均值变成0，标准差变成1。就像给每个数据打分，告诉你它离平均值有多远。</p><p><strong>数学公式：</strong><br>$x^* &#x3D; \frac{x - \mu}{\sigma}$</p><ul><li>$x$ 是原始数据的值。</li><li>$\mu$ 是数据的平均值（均值）。</li><li>$\sigma$ 是数据的标准差（衡量数据分散程度）。</li><li>$x^*$ 是标准化后的新值。</li></ul><p><strong>怎么理解这个公式？</strong></p><ul><li>减去 $\mu$：把数据中心移到0。</li><li>除以 $\sigma$：把数据的分散程度调整到1。</li><li>结果：数据变成“标准正态分布”的样子，均值是0，标准差是1。</li></ul><p><strong>举个例子：</strong><br>假设你的数据是 <code>[10, 20, 30, 40, 50]</code>：</p><ul><li>均值 $\mu &#x3D; (10 + 20 + 30 + 40 + 50) &#x2F; 5 &#x3D; 30$。</li><li>标准差 $\sigma \approx 15.81$（通过计算得来）。</li><li>对于 $x &#x3D; 20$：<br>$x^* &#x3D; \frac{20 - 30}{15.81} &#x3D; \frac{-10}{15.81} \approx -0.63$</li><li>结果：<code>[-1.26, -0.63, 0, 0.63, 1.26]</code>。</li></ul><p><strong>图片里说了什么？</strong></p><ul><li>Z-Score标准化让数据均值为0，标准差为1，方便在不同特征间比较。</li><li>它不会改变数据的分布形状，只是移动和缩放了一下（比如原来是正态分布的，还是正态分布）。</li></ul><p><strong>什么时候用？</strong><br>当你的数据有明显的大小差异，或者模型（像线性回归）假设数据是标准化的，Z-Score就很合适。它对异常值也更稳健。</p><h4 id="3-两者的区别和联系"><a href="#3-两者的区别和联系" class="headerlink" title="3. 两者的区别和联系"></a>3. 两者的区别和联系</h4><table><thead><tr><th><strong>方面</strong></th><th><strong>归一化</strong></th><th><strong>Z-Score标准化</strong></th></tr></thead><tbody><tr><td><strong>目标范围</strong></td><td>数据缩放到[0,1]</td><td>均值为0，标准差为1</td></tr><tr><td><strong>公式</strong></td><td>$\frac{x - x_{\text{min}}}{x_{\text{max}} - x_{\text{min}}}$</td><td>$\frac{x - \mu}{\sigma}$</td></tr><tr><td><strong>分布变化</strong></td><td>可能改变分布形状（扩展&#x2F;压缩）</td><td>不改变分布形状</td></tr><tr><td><strong>适用场景</strong></td><td>需要统一范围的模型</td><td>需要比较距离或正态化的模型</td></tr></tbody></table><p><strong>图片里的观察：</strong></p><ul><li>归一化像“强行统一规格”，可能会让数据分布变形。</li><li>Z-Score像“调整视角”，保持数据原本的样子，只是换了个标准来看。</li></ul><hr><h4 id="4-为什么要做数据标准化？"><a href="#4-为什么要做数据标准化？" class="headerlink" title="4. 为什么要做数据标准化？"></a>4. 为什么要做数据标准化？</h4><p>机器学习模型就像挑食的孩子，如果数据尺度差别太大（比如一个特征是0-1000，另一个是0-1），模型可能会“偏心”，只关注数值大的特征。标准化通过归一化或Z-Score，把所有特征拉到同一水平线上，让模型公平对待每个特征，提升预测效果。</p><p><strong>实际意义举例：</strong></p><ul><li>预测房价时，面积（平米）和房间数（个）单位不同，归一化或Z-Score能让它们平等竞争。</li><li>在图像处理中，像素值（0-255）如果不标准化，可能让模型训练变慢或不稳定。</li></ul><h3 id="2-特征二值化"><a href="#2-特征二值化" class="headerlink" title="2. 特征二值化"></a>2. 特征二值化</h3><p>目的是通过对数据进行处理，将数值型特征转化为二进制特征（0或1），从而优化数据以提升模型性能。</p><hr><h4 id="2-什么是特征二值化？"><a href="#2-什么是特征二值化？" class="headerlink" title="2. 什么是特征二值化？"></a>2. 什么是特征二值化？</h4><p>特征二值化是一种将数值型数据转换为二进制数据（0或1）的技术。具体来说：</p><ul><li>设定一个<strong>阈值</strong>（threshold）。</li><li>如果特征值<strong>大于或等于阈值</strong>，就标记为<strong>1</strong>。</li><li>如果特征值<strong>小于阈值</strong>，就标记为<strong>0</strong>。</li></ul><p><strong>通俗解释</strong>：<br>就像考试及格线是60分，成绩≥60分的标记为“及格”（1），&lt;60分的标记为“不及格”（0）。这里的60就是阈值。</p><p><strong>目的</strong>：</p><ul><li>简化数据：将复杂的数值转化为简单的二进制形式。</li><li>突出特征的有无：某些场景下，特征是否超过某个值比具体数值更重要。</li></ul><hr><h4 id="3-图片中的代码示例"><a href="#3-图片中的代码示例" class="headerlink" title="3. 图片中的代码示例"></a>3. 图片中的代码示例</h4><p><strong>代码解释</strong>：</p><ul><li><p>**<code>from sklearn.preprocessing import Binarizer</code>**：从Scikit-learn的预处理模块中导入<code>Binarizer</code>类。</p></li><li><p>**<code>Binarizer(threshold=3)</code>**：创建一个二值化工具，设置阈值为3。</p></li><li><p><code>.fit_transform(iris.data)</code></p><p>：将这个二值化操作应用到Iris数据集</p><ul><li>Iris数据集包含花瓣和花萼的长度&#x2F;宽度等特征，单位是厘米。</li><li>对于每个特征值：≥3的变成1，&lt;3的变成0。</li></ul></li></ul><p><strong>举个例子</strong>：<br>假设Iris数据集中的某个特征值是 <code>[1.5, 3.2, 4.0, 2.8]</code>，阈值设为3：</p><ul><li>1.5 &lt; 3 → 0</li><li>3.2 ≥ 3 → 1</li><li>4.0 ≥ 3 → 1</li><li>2.8 &lt; 3 → 0<br>结果：<code>[0, 1, 1, 0]</code>。</li></ul><p><strong>注意</strong>：<br>图片中的阈值3只是一个示例。实际中，Iris数据集的特征值大多大于3（比如花瓣长度通常在1-6厘米之间），所以阈值3可能不是最优选择。阈值应根据具体问题和数据分布来设定。</p><hr><h4 id="4-特征二值化的应用场景"><a href="#4-特征二值化的应用场景" class="headerlink" title="4. 特征二值化的应用场景"></a>4. 特征二值化的应用场景</h4><ul><li><strong>简化模型</strong>：在某些情况下，模型不需要精确的数值，只需要知道特征是否超过某个关键点。比如在垃圾邮件检测中，一个词的出现次数超过某个值（比如3次）可能就足以判断它是重要的，而不需要具体次数。</li><li><strong>减少计算复杂度</strong>：二值化后的0和1更简单，模型训练可能更快。</li><li><strong>突出关键信息</strong>：比如在用户行为分析中，登录次数≥5次标记为“活跃”（1），&lt;5次标记为“不活跃”（0）。</li></ul><hr><h4 id="5-特征二值化的优缺点"><a href="#5-特征二值化的优缺点" class="headerlink" title="5. 特征二值化的优缺点"></a>5. 特征二值化的优缺点</h4><p><strong>优点</strong>：</p><ul><li>数据简化，易于处理。</li><li>可以突出特征是否达到某个阈值，适合某些分类问题。</li></ul><p><strong>缺点</strong>：</p><ul><li>信息损失：把连续值变成0或1，丢掉了原始数据的细节。</li><li>阈值选择关键：选得不合适，可能影响模型效果。</li></ul><h3 id="3-定性特征哑编码"><a href="#3-定性特征哑编码" class="headerlink" title="3. 定性特征哑编码"></a>3. <strong>定性特征哑编码</strong></h3><p><strong>定性特征哑编码（也称为独热编码，One-Hot Encoding）</strong>。它包含了一段Python代码，演示了如何使用Scikit-learn库中的<code>OneHotEncoder</code>类对数据进行哑编码。下面是对图片内容的详细解释，帮助你理解它的含义和作用。</p><hr><h4 id="1-图片的主题和文本"><a href="#1-图片的主题和文本" class="headerlink" title="1. 图片的主题和文本"></a>1. 图片的主题和文本</h4><ul><li><p>标题</p><p>：定性特征哑编码</p><ul><li>这表明图片的重点是介绍一种将定性（类别型）特征转化为数值形式的技术。</li></ul></li><li><p>文本</p><p>：使用preprocessing库的OneHotEncoder类对数据进行哑编码的代码如下：</p><ul><li>这部分提示我们，接下来会展示一段具体的代码示例，使用的工具是Scikit-learn中的<code>OneHotEncoder</code>类。</li></ul></li></ul><hr><h4 id="2-代码示例"><a href="#2-代码示例" class="headerlink" title="2. 代码示例"></a>2. 代码示例</h4><p><strong>代码逐步拆解</strong>：</p><ul><li><p><strong><code>from sklearn.preprocessing import OneHotEncoder</code></strong></p><ul><li>从Scikit-learn的<code>preprocessing</code>模块中导入<code>OneHotEncoder</code>类。这个类是专门用来做独热编码的工具。</li></ul></li><li><p><strong><code>iris.target</code></strong></p><ul><li><pre><code>iris.target<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">    是Iris数据集中的目标值，也就是花的类别标签。Iris数据集是一个经典的机器学习数据集，包含三种花的类别：</span><br><span class="line"></span><br><span class="line">    - 0：Setosa</span><br><span class="line">    - 1：Versicolor</span><br><span class="line">    - 2：Virginica</span><br><span class="line"></span><br><span class="line">  - 原始形式是一个一维数组，例如 `[0, 1, 2, 0, 1, ...]`，表示150个样本的类别。</span><br><span class="line"></span><br><span class="line">- **`.reshape((-1,1))`**</span><br><span class="line"></span><br><span class="line">  - `OneHotEncoder`要求输入数据是二维的，但`iris.target`是一维数组。为了适配要求，这里用`reshape((-1,1))`将其变成一个二维数组（列向量）。</span><br><span class="line">  - `-1`表示自动计算行数（这里是150行），`1`表示1列。</span><br><span class="line">  - 转换后，数据从 `[0, 1, 2]` 变成 `[[0], [1], [2]]`。</span><br><span class="line"></span><br><span class="line">- **`OneHotEncoder().fit_transform()`**</span><br><span class="line"></span><br><span class="line">  - `OneHotEncoder()`：创建一个独热编码的对象。</span><br><span class="line">  - `.fit_transform()`：对数据进行拟合（fit）和转换（transform），一步完成编码过程。</span><br><span class="line">  - 输出是哑编码后的结果，通常是一个稀疏矩阵（sparse matrix），表示每个样本的类别用二进制向量表示。</span><br><span class="line"></span><br><span class="line">------</span><br><span class="line"></span><br><span class="line">#### 3. 哑编码（独热编码）的原理</span><br><span class="line"></span><br><span class="line">**什么是独热编码？**</span><br><span class="line">独热编码是一种将类别型数据（比如“红”、“蓝”、“绿”）转化为数值形式的方法。它的核心思路是：</span><br><span class="line"></span><br><span class="line">- 为每个类别创建一个单独的二进制特征（列）。</span><br><span class="line">- 如果样本属于某个类别，该列值为1，其他列值为0。</span><br><span class="line"></span><br><span class="line">**举个例子**：</span><br><span class="line">Iris数据集的目标值有三种类别：0（Setosa）、1（Versicolor）、2（Virginica）。独热编码后：</span><br><span class="line"></span><br><span class="line">- 类别0 → `[1, 0, 0]`</span><br><span class="line">- 类别1 → `[0, 1, 0]`</span><br><span class="line">- 类别2 → `[0, 0, 1]`</span><br><span class="line"></span><br><span class="line">编码后，每个样本的类别被表示为一个三维向量，清楚地告诉模型“这是哪一类”。</span><br><span class="line"></span><br><span class="line">**代码的实际效果**：</span><br><span class="line"></span><br><span class="line">- 输入：`iris.target`，一个包含150个样本类别（0, 1, 2）的一维数组。</span><br><span class="line"></span><br><span class="line">- 处理：通过`reshape((-1,1))`变成150行1列的二维数组，再用`OneHotEncoder`编码。</span><br><span class="line"></span><br><span class="line">- 输出：一个150行3列的稀疏矩阵，每行是一个样本的独热编码向量。例如：</span><br><span class="line"></span><br></pre></td></tr></table></figure>lua复制原数据: [0, 1, 2]编码后:[[1, 0, 0], [0, 1, 0], [0, 0, 1]]<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">------</span><br><span class="line"></span><br><span class="line">#### 4. 为什么要做独热编码？</span><br><span class="line"></span><br><span class="line">- **模型需求**：机器学习模型（比如线性回归、神经网络）只能处理数值数据，而不能直接理解类别型数据（比如“Setosa”）。独热编码把类别变成数字，让模型能用。</span><br><span class="line">- **避免误解**：如果直接把类别编码为0、1、2，模型可能会认为2比1大，1比0大，误以为类别间有大小关系。独热编码用二进制向量避免了这种问题。</span><br><span class="line"></span><br><span class="line">------</span><br><span class="line"></span><br><span class="line">#### 5. 应用场景和注意事项</span><br><span class="line"></span><br><span class="line">**应用场景**：</span><br><span class="line"></span><br><span class="line">- **分类任务**：像Iris数据集这样，目标值是类别型的，需要独热编码。</span><br><span class="line">- **特征处理**：数据中有类别型特征（比如性别、颜色），需要转为数值形式。</span><br><span class="line"></span><br><span class="line">**注意事项**：</span><br><span class="line"></span><br><span class="line">- **维度增加**：如果类别很多（比如100种颜色），独热编码会生成100列，增加数据维度，可能导致计算负担重。</span><br><span class="line">- **稀疏矩阵**：`OneHotEncoder`默认返回稀疏矩阵格式（节省内存），实际使用时可能需要转成普通数组（用`.toarray()`）。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### 4. 分箱</span><br><span class="line"></span><br><span class="line">给出了两种常见的分箱方法：**按区间分箱**和**按数量分箱**。下面是对图片内容的详细解释，帮助你理解它的含义和应用。</span><br><span class="line"></span><br><span class="line">------</span><br><span class="line"></span><br><span class="line">#### 1. 图片的主题</span><br><span class="line"></span><br><span class="line">图片的核心主题是**数据分箱**，这是一种数据预处理技术，主要用于将连续型变量（比如成绩、年龄等）转化为离散的类别型特征。文字说明：</span><br><span class="line"></span><br><span class="line">&gt; 一般在建立分类模型时，需要对连续变量离散化，特别离散化后，模型会更稳定，降低了模型过拟合的风险。</span><br><span class="line"></span><br><span class="line">**为什么要做分箱？**</span><br><span class="line"></span><br><span class="line">- **模型稳定性**：离散化后的特征可以减少数据中的噪声影响，使模型更稳健。</span><br><span class="line">- **降低过拟合**：将连续值分组后，模型不会过于关注具体的数值差异，从而提高泛化能力。</span><br><span class="line"></span><br><span class="line">------</span><br><span class="line"></span><br><span class="line">#### 2. 数据示例</span><br><span class="line"></span><br><span class="line">图片中给出了一个具体的连续型数据——学生的成绩列表：</span><br><span class="line"></span><br></pre></td></tr></table></figure></code></pre></li></ul></li></ul><p>csharp</p><p>复制<br>[63, 64, 88, 71, 42, 60, 99, 70, 32, 88, 34, 69, 83, 52, 66, 92, 82, 58, 66, 41]</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">这个列表包含20个成绩，范围从32到99。我们将通过两种分箱方法对它进行处理。</span><br><span class="line"></span><br><span class="line">------</span><br><span class="line"></span><br><span class="line">#### 3. 分箱方法一：按区间分箱（使用 `pd.cut`）</span><br><span class="line"></span><br><span class="line">**什么是按区间分箱？**</span><br><span class="line">这种方法是手动设定分箱的边界（区间），然后将数据划分到这些区间中。就像给成绩划分等级：不及格（0-59）、及格（60-70）、良好（71-80）等。</span><br><span class="line"></span><br><span class="line">**代码示例：**</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>python复制bins &#x3D; [0, 59, 70, 80, 90, 100]<br>score_cat &#x3D; pd.cut(score_list, bins)<br>print(pd.value_counts(score_cat))</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- **`bins`**：定义了分箱的边界点：[0, 59, 70, 80, 90, 100]，表示5个区间：(0-59], (59-70], (70-80], (80-90], (90-100]。</span><br><span class="line">- **`pd.cut`**：将成绩列表按这些区间划分。</span><br><span class="line">- **`pd.value_counts`**：统计每个区间内的数据数量。</span><br><span class="line"></span><br><span class="line">**输出结果：**</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>复制(59, 70]    7<br>(0, 59]     6<br>(80, 90]    4<br>(90, 100]   2<br>(70, 80]    1  </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">**解释输出：**</span><br><span class="line"></span><br><span class="line">- **(59, 70]**：7个成绩在这个区间，比如63、64、60等。</span><br><span class="line">- **(0, 59]**：6个成绩在这个区间，比如42、32、34等。</span><br><span class="line">- **(80, 90]**：4个成绩在这个区间，比如88、88、83、82。</span><br><span class="line">- **(90, 100]**：2个成绩在这个区间，比如99、92。</span><br><span class="line">- **(70, 80]**：1个成绩在这个区间，比如71。</span><br><span class="line"></span><br><span class="line">**特点：**</span><br><span class="line"></span><br><span class="line">- 区间宽度可以自定义（比如0-59是59个单位，59-70是11个单位），适合根据业务需求（如成绩等级）划分。</span><br><span class="line">- 每个区间的数量可能不均匀，比如(59, 70]有7个数据，而(70, 80]只有1个。</span><br><span class="line"></span><br><span class="line">------</span><br><span class="line"></span><br><span class="line">#### 4. 分箱方法二：按数量分箱（使用 `pd.qcut`）</span><br><span class="line"></span><br><span class="line">**什么是按数量分箱？**</span><br><span class="line">这种方法是自动将数据按数量均分成若干份（等频分箱），每个区间包含相同数量的数据点。就像把学生按成绩高低分成5个小组，每组人数一样。</span><br><span class="line"></span><br><span class="line">**代码示例：**</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>python复制score_cat &#x3D; pd.qcut(score_list, 5)<br>print(pd.value_counts(score_cat))</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- **`pd.qcut`**：将成绩列表分成5个区间，每个区间尽量包含相同数量的数据（这里是20个数据，分成5份，每份4个）。</span><br><span class="line">- **`5`**：指定分成5个区间。</span><br><span class="line"></span><br><span class="line">**输出结果：**</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>复制(31.999, 50.0]    4<br>(50.0, 63.6]      4<br>(63.6, 69.4]      4<br>(69.4, 84.0]      4<br>(84.0, 99.0]      4  </p><pre><code>**解释输出：**- **(31.999, 50.0]**：包含4个最低的成绩，比如32、34、42、41。- **(50.0, 63.6]**：包含接下来的4个成绩，比如52、58、60、63。- **(63.6, 69.4]**：包含中间的4个成绩，比如64、66、66、69。- **(69.4, 84.0]**：包含较高的4个成绩，比如70、71、82、83。- **(84.0, 99.0]**：包含最高的4个成绩，比如88、88、92、99。**特点：**- 每个区间的数据量相等（这里都是4个），适合数据分布不均匀时，确保每组都有代表性。- 区间宽度可能不同，比如(31.999, 50.0]跨度18，而(63.6, 69.4]只有5.8。------#### 5. 两种方法的对比| **方面**     | **按区间分箱 (`pd.cut`)**    | **按数量分箱 (`pd.qcut`)** || ------------ | ---------------------------- | -------------------------- || **划分依据** | 手动设定的固定区间           | 自动按数据量均分           || **区间数量** | 不一定相等（比如7 vs 1）     | 相等（这里都是4）          || **区间宽度** | 可自定义，可能不均           | 自动调整，可能不均         || **适用场景** | 有明确业务规则（如成绩等级） | 数据分布未知，需等频分析   |------#### 6. 分箱的意义- **离散化**：将连续的成绩（32到99）变成类别（比如“低分”、“中分”、“高分”），方便分类模型处理。- **捕捉模式**：分组后，模型能更容易发现成绩分布的规律，而不是纠结于具体数值。- **实际应用**：比如在信用评分中，把收入分箱成“低收入”、“中等收入”、“高收入”，帮助模型预测违约风险。------#### 7. 总结这张图片通过一个成绩数据集，展示了数据分箱的两种方法：- **按区间分箱 (`pd.cut`)**：根据手动设定的边界划分，适合有明确规则的场景。- **按数量分箱 (`pd.qcut`)**：自动按数量均分，适合数据分布不明的场景。### 5. 聚合特征构造#### 1. 什么是聚合特征构造？**定义**：聚合特征构造是一种通过对多个特征进行**分组聚合**来创建新特征的方法。这些特征通常来自同一张表格，或者通过多张表格的联接（JOIN操作）得到的联合数据集。**通俗解释**：想象你有一张记录顾客购买情况的表格，里面有顾客ID、购买日期和购买金额等信息。聚合特征构造就像是按顾客ID把数据“打包”，然后算出每个顾客的平均购买金额、最高金额等统计数据，这些统计数据就是新的特征。**图片中的描述**：- “聚合特征构造主要通过对多个特征的分组聚合实现，这些特征通常来自同一张表或者多张表的联立。”  - 这说明聚合特征构造的核心是对数据进行分组，并从分组中提取有用的信息。------#### 2. 聚合特征构造的实现方式**步骤**：聚合特征构造通常基于**一对多的关联**（One-to-Many Relationship），具体分为两步：1. **分组**：根据某个键（比如顾客ID）将数据分成若干组。2. **计算统计量**：对每组数据计算一些统计指标，生成新特征。**通俗例子**：假设你有一张订单表：| 顾客ID | 订单日期   | 订单金额 || ------ | ---------- | -------- || A      | 2023-01-01 | 100      || A      | 2023-02-01 | 200      || B      | 2023-01-15 | 150      |- **一对多关联**：顾客A有2条订单记录，顾客B有1条，体现了一（顾客）对多（订单）的关系。- **分组**：按顾客ID分组，A组包含金额[100, 200]，B组包含金额[150]。- 计算统计量  ：  - A的平均金额 = (100 + 200) / 2 = 150  - A的最大金额 = 200  - B的平均金额 = 150- **新特征**：生成一张新表：| 顾客ID | 平均金额 | 最大金额 || ------ | -------- | -------- || A      | 150      | 200      || B      | 150      | 150      |**图片中的描述**：- “聚合特征构造使用一对多的关联来对观测值分组，然后计算统计量。”  - 这清晰地概括了分组和统计的过程。------#### 3. 常见的统计量图片列举了一些常见的分组统计量，这些统计量可以作为新特征，用于描述数据的特性：- **中位数（Median）**：组内数据的中间值，比如A组[100, 200]的中位数是150。- **算术平均数（Mean）**：组内数据的平均值，比如A组的平均金额是150。- **众数（Mode）**：组内出现最频繁的值（如果数据量少，可能不适用）。- **最大值（Max）**：组内最大的值，比如A组的最大金额是200。- **最小值（Min）**：组内最小的值，比如A组的最小金额是100。- **标准差（Standard Deviation）**：衡量组内数据的离散程度，反映波动大小。- **方差（Variance）**：类似标准差，衡量数据的波动情况。- **频数（Count）**：组内记录的数量，比如A组有2条记录，频数是2。**实际应用**：- 在电商中，可以用“平均购买金额”反映顾客的消费能力，用“购买频数”反映活跃度。- 在金融中，可以用“交易金额的标准差”判断用户的消费稳定性。------#### 4. 聚合特征构造的意义- **丰富特征**：从原始数据中挖掘更多信息，让模型能捕捉到更深层次的模式。- **降维**：把多条记录汇总成几个统计量，减少数据量，提高计算效率。- **捕捉趋势**：比如通过“最大金额”发现高端消费行为，通过“频数”发现用户忠诚度。------#### 5. 总结这张图片简洁地介绍了**聚合特征构造**的概念和实现方式：- **定义**：通过分组聚合从数据中创建新特征。- **方法**：利用一对多关联分组，然后计算统计量。- **统计量**：包括中位数、平均数、众数、最大值、最小值、标准差、方差和频数等。- **价值**：提升模型性能，增强数据表达能力。### 6. PCAPCA 就像给数据找一个“最佳视角”。想象你有一堆点分布在三维空间（比如房屋的面积、房间数、楼层数），这些点散乱得像天上的星星。PCA 能找到一条线或一个平面，把这些点“投影”过去，让你用更少的维度（比如从3D变2D）看清楚它们的大致分布。**核心目标**：- 减少特征数量（降维）。- 保留数据的主要信息（变化最大的部分）。那它是怎么做到的呢？下面是 PCA 的数学实现步骤，用生活化的例子带你走一遍。------#### PCA 的数学实现步骤##### **步骤 1：把数据“搬到原点”——中心化处理****做什么？**我们先让数据的平均值变成0。怎么做呢？对每个特征（比如面积、房间数），算出它的平均值，然后从每个数据点里减掉这个平均值。**数学公式：**$X_&#123;\text&#123;centered&#125;&#125; = X - \mu$- $X$ 是原始数据（比如一个表格，每行是一个房子，每列是一个特征）。- $\mu$ 是每个特征的平均值（比如所有房子的平均面积）。- $X_&#123;\text&#123;centered&#125;&#125;$ 是中心化后的数据。**举个例子：**假设有3个房子的面积数据：- 原始数据：`[1500, 1800, 2100]`- 平均值：$\mu = (1500 + 1800 + 2100) / 3 = 1800$- 中心化后：`[1500-1800, 1800-1800, 2100-1800] = [-300, 0, 300]`**为什么这样做？**把数据移到原点（均值为0），就像把散乱的点都挪到坐标系中间，方便我们找它们变化的方向。------##### **步骤 2：看看数据怎么“散”——计算协方差矩阵****做什么？**中心化后，我们要知道数据是怎么“散开”的。协方差矩阵就像一个“地图”，告诉你每个特征自己变了多少（方差），还有特征之间怎么一起变（协方差）。**数学公式：**$C = \frac&#123;1&#125;&#123;n&#125; \cdot X_&#123;\text&#123;centered&#125;&#125;^T \cdot X_&#123;\text&#123;centered&#125;&#125;$- $n$ 是数据点的数量（比如3个房子）。- $X_&#123;\text&#123;centered&#125;&#125;^T$ 是中心化数据的转置（行变列，列变行）。- $C$ 是协方差矩阵。**举个例子：**假设有2个特征：面积和房间数，中心化后的数据是：| 面积 | 房间数 || ---- | ------ || -300 | -1     || 0    | 0      || 300  | 1      |- $X_&#123;\text&#123;centered&#125;&#125;$ 是一个 3×2 的矩阵。- 计算   XcenteredT⋅XcenteredX_&#123;\text&#123;centered&#125;&#125;^T \cdot X_&#123;\text&#123;centered&#125;&#125;XcenteredT⋅Xcentered  ，结果是一个 2×2 的矩阵，再除以   n=3n=3n=3  ，得到：  C=[600002002000.6667]C = \begin&#123;bmatrix&#125; 60000 &amp; 200 \\ 200 &amp; 0.6667 \end&#123;bmatrix&#125;C=[600002002000.6667]  - 对角线：面积的方差（60000），房间数的方差（0.6667）。  - 非对角线：面积和房间数的协方差（200），说明它们有点相关。**为什么这样做？**协方差矩阵告诉我们数据在哪个方向变化最大，这些方向就是我们想要的主成分。------##### **步骤 3：找到“变化最大的方向”——特征值分解****做什么？**对协方差矩阵做“特征值分解”，找出它的特征值和特征向量。- **特征值（λ）**：告诉你每个方向的变化量（方差）有多大。- **特征向量（v）**：告诉你这些方向具体是啥。**数学公式：**$C \cdot v = \lambda \cdot v$- $C$ 是协方差矩阵。- $v$ 是特征向量，$\lambda$ 是特征值。**举个例子：**对上面的协方差矩阵 $C$ 做分解，可能得到：- 特征值：$\lambda_1 = 60001, \lambda_2 = 0.3333$- 特征向量：  - $v_1 = [0.999, 0.003]$（第一个主成分方向）  - $v_2 = [-0.003, 0.999]$（第二个主成分方向）**为什么这样做？**- 特征值大的方向（比如 $\lambda_1 = 60001$）是数据变化最大的方向，信息最多。- 特征向量是新坐标轴的方向，我们要用它们重新表示数据。------##### **步骤 4：挑最重要的方向，投影数据——降维****做什么？**从特征值中挑最大的几个（比如前k个），用对应的特征向量把原始数据“投影”过去，得到低维数据。**数学公式：**$Y = X_&#123;\text&#123;centered&#125;&#125; \cdot V_k$- $V_k$ 是前k个特征向量组成的矩阵（比如 2×k）。- $Y$ 是降维后的数据（行数不变，列数变成k）。**举个例子：**- 假设我们只取第一个主成分（$k=1$），$V_1 = [0.999, 0.003]$。- 中心化数据 $X_&#123;\text&#123;centered&#125;&#125;$ 是 3×2 的矩阵：  $\begin&#123;bmatrix&#125; -300 &amp; -1 \\ 0 &amp; 0 \\ 300 &amp; 1 \end&#123;bmatrix&#125;$- 投影：$Y = X_&#123;\text&#123;centered&#125;&#125; \cdot V_1 = [-299.7, 0, 300.3]$- 结果：原来是2维（面积和房间数），现在是1维。**为什么这样做？**- 挑最大的特征值对应的方向，能保留最多信息（比如这里保留了99.9%的方差）。- 丢掉小的方向，降低维度，简化计算。------#### 一个完整的例子假设我们有3个房子的数据：| 面积 (平米) | 房间数 || ----------- | ------ || 1500        | 3      || 1800        | 4      || 2100        | 3      |1. **中心化**：   - 面积均值：1800，房间数均值：3.33   - 中心化后：`[-300, -0.33], [0, 0.67], [300, -0.33]`2. **协方差矩阵**：   $C = \begin&#123;bmatrix&#125; 60000 &amp; 50 \\ 50 &amp; 0.222 \end&#123;bmatrix&#125;$3. **特征值分解**：   - 特征值：$\lambda_1 = 60000.2, \lambda_2 = 0.02$   - 特征向量：$v_1 = [0.999, 0.001], v_2 = [-0.001, 0.999]$4. **投影**：   - 只取 $v_1$，新数据：`[-299.7, 0.67, 299.7]`   - 从2维降到1维，保留了几乎全部信息。------#### PCA 的意义- **降维**：从一大堆特征变成几个关键特征，计算更快。- **保留信息**：挑变化最大的方向，保证不丢关键东西。- **应用**：比如把几百个特征降到2个，还能画图看看数据长啥样。------#### 总结PCA 的数学实现就像一场“数据瘦身运动”：1. **中心化**：把数据搬到原点。2. **协方差矩阵**：看看数据怎么散。3. **特征值分解**：找到变化最大的方向。4. **投影**：把数据压到这些方向上，瘦身成功！### 7. ICA#### 1. ICA是什么？**定义**：独立成分分析（ICA）是一种通过线性变换从混合数据中提取独立成分的统计方法。它的目标是将观测到的混合信号分解成多个独立的源信号。**通俗解释**：想象你在鸡尾酒会上，房间里有很多人同时说话，几个麦克风录下了这些声音的混合信号。ICA就像一个“超级听力专家”，能从这些杂乱的录音中分辨出每个人的声音，把它们分开。**核心思想**：假设我们观测到的数据（如声音、图像）是由多个独立信号混合而成的，ICA通过数学方法找到一个变换，把这些混合信号“解混”，恢复出原始的独立信号。------#### 2. ICA的数学实现ICA的核心是通过线性变换来实现信号分离，具体公式如下：**基本公式**：$\mathbf&#123;z&#125; = \mathbf&#123;W&#125; \cdot \mathbf&#123;x&#125;$- $\mathbf&#123;x&#125;$：观测到的混合数据（输入数据），通常是一个矩阵，每一行是一个样本，每一列是一个特征。- $\mathbf&#123;W&#125;$：权重矩阵（也叫解混矩阵），这是我们要通过算法求解的关键。- $\mathbf&#123;z&#125;$：转换后的结果，表示提取出的独立成分，每一行是一个样本，每一列是一个独立的源信号。**目标**：找到一个合适的 $\mathbf&#123;W&#125;$，使得 $\mathbf&#123;z&#125;$ 中的各个成分（$\mathbf&#123;z&#125;$ 的每一列）之间的**统计独立性**最大化。统计独立性比简单的“无关”（如PCA中的去相关性）更强，指的是各个成分之间没有任何统计上的依赖关系。**假设**：- 源信号是**非高斯分布**的（因为高斯信号混合后无法分离）。- 源信号之间是**独立**的。------#### 3. ICA的具体流程图片中提到，**PCA是ICA的预处理方法**。以下是ICA的完整流程，结合数学实现：##### **步骤1：数据预处理——使用PCA降维**- **为什么用PCA？**  - PCA可以去除特征间的相关性，为ICA提供更“干净”的数据（ICA假设源信号独立，而相关性会干扰这一假设）。  - PCA还能减少数据维度，降低计算复杂度。- **数学实现**：  1. **中心化**：对输入数据 $\mathbf&#123;x&#125;$ 进行中心化，使每个特征的均值为0。     $\mathbf&#123;x&#125;_&#123;\text&#123;centered&#125;&#125; = \mathbf&#123;x&#125; - \text&#123;mean&#125;(\mathbf&#123;x&#125;)$  2. **计算协方差矩阵**：     $\mathbf&#123;C&#125; = \frac&#123;1&#125;&#123;n&#125; \cdot \mathbf&#123;x&#125;_&#123;\text&#123;centered&#125;&#125;^T \cdot \mathbf&#123;x&#125;_&#123;\text&#123;centered&#125;&#125;$ 其中 $n$ 是样本数。  3. **特征值分解**：对 $\mathbf&#123;C&#125;$ 进行分解，得到特征值 $\lambda_i$ 和特征向量 $\mathbf&#123;v&#125;_i$。     $\mathbf&#123;C&#125; \cdot \mathbf&#123;v&#125;_i = \lambda_i \cdot \mathbf&#123;v&#125;_i$  4. 白化（Whitening）     ：将数据投影到特征向量上，并除以特征值的平方根，使数据方差归一化。     xwhitened=E⋅D−1/2⋅ET⋅xcentered\mathbf&#123;x&#125;_&#123;\text&#123;whitened&#125;&#125; = \mathbf&#123;E&#125; \cdot \mathbf&#123;D&#125;^&#123;-1/2&#125; \cdot \mathbf&#123;E&#125;^T \cdot \mathbf&#123;x&#125;_&#123;\text&#123;centered&#125;&#125;xwhitened=E⋅D−1/2⋅ET⋅xcentered     - $\mathbf&#123;E&#125;$：特征向量矩阵。     - $\mathbf&#123;D&#125;$：特征值对角矩阵。- **结果**：$\mathbf&#123;x&#125;_&#123;\text&#123;whitened&#125;&#125;$ 是白化后的数据，特征间无相关性，方差为1，作为ICA的输入。##### **步骤2：ICA分离独立成分**- **目标**：在白化数据 $\mathbf&#123;x&#125;_&#123;\text&#123;whitened&#125;&#125;$ 上找到解混矩阵 $\mathbf&#123;W&#125;$，使 $\mathbf&#123;z&#125; = \mathbf&#123;W&#125; \cdot \mathbf&#123;x&#125;_&#123;\text&#123;whitened&#125;&#125;$ 的各个成分独立。- **独立性度量**：  - **互信息（Mutual Information）**：衡量变量间的依赖性，互信息越小，独立性越高。    $I(\mathbf&#123;z&#125;_i, \mathbf&#123;z&#125;_j) = H(\mathbf&#123;z&#125;_i) + H(\mathbf&#123;z&#125;_j) - H(\mathbf&#123;z&#125;_i, \mathbf&#123;z&#125;_j)$ 其中 $H$ 是熵，目标是让 $I = 0$。  - **负熵（Negentropy）**：衡量数据的非高斯性（因为独立信号通常是非高斯的）。    $J(\mathbf&#123;z&#125;) = H(\mathbf&#123;z&#125;_&#123;\text&#123;gaussian&#125;&#125;) - H(\mathbf&#123;z&#125;)$ 负熵越大，非高斯性越强，独立性越好。- **优化方法**：  使用迭代算法调整 $\mathbf&#123;W&#125;$，最大化独立性。常见算法包括：  - FastICA    ：通过最大化负熵快速分离独立成分。    1. 随机初始化 $\mathbf&#123;W&#125;$。    2. 计算 $\mathbf&#123;z&#125; = \mathbf&#123;W&#125; \cdot \mathbf&#123;x&#125;_&#123;\text&#123;whitened&#125;&#125;$。    3. 更新 $\mathbf&#123;W&#125;$：       $\mathbf&#123;W&#125; \leftarrow E\&#123;\mathbf&#123;x&#125;_&#123;\text&#123;whitened&#125;&#125; \cdot g(\mathbf&#123;W&#125; \cdot \mathbf&#123;x&#125;_&#123;\text&#123;whitened&#125;&#125;)\&#125; - E\&#123;g&#39;(\mathbf&#123;W&#125; \cdot \mathbf&#123;x&#125;_&#123;\text&#123;whitened&#125;&#125;)\&#125; \cdot \mathbf&#123;W&#125;$ 其中 $g$ 是非线性函数（如 $g(y) = \tanh(y)$），$E$ 是期望。    4. 正交化 $\mathbf&#123;W&#125;$（保持独立性），重复直到收敛。  - **Infomax**：通过最大化信息传输实现分离。- **结果**：$\mathbf&#123;z&#125;$ 是分离出的独立成分。------#### 4. ICA与PCA的区别- PCA（主成分分析）  ：  - 目标：最大化数据的方差。  - 方法：找到方差最大的正交方向。  - 结果：去相关性，但不保证独立性。- ICA  ：  - 目标：最大化特征间的独立性。  - 方法：基于非高斯性和统计独立性。  - 结果：分离出独立的源信号。**例子**：如果混合信号是两个人的声音，PCA可能会找到声音的“主要变化方向”，但不能分开两个人；ICA则能直接分离出每个人的声音。------#### 5. 数学实现的例子假设我们有2个混合信号（$\mathbf&#123;x&#125;$，2维数据），来自2个独立源信号（$\mathbf&#123;s&#125;$）：- $\mathbf&#123;x&#125;_1 = 0.5 \cdot \mathbf&#123;s&#125;_1 + 0.3 \cdot \mathbf&#123;s&#125;_2$- $\mathbf&#123;x&#125;_2 = 0.2 \cdot \mathbf&#123;s&#125;_1 + 0.6 \cdot \mathbf&#123;s&#125;_2$**目标**：找到 $\mathbf&#123;W&#125;$，使 $\mathbf&#123;z&#125; = \mathbf&#123;W&#125; \cdot \mathbf&#123;x&#125;$ 恢复 $\mathbf&#123;s&#125;$。1. **预处理**：   - 中心化：减去 $\mathbf&#123;x&#125;$ 的均值。   - 白化：用PCA将 $\mathbf&#123;x&#125;$ 转换为无相关性的 $\mathbf&#123;x&#125;_&#123;\text&#123;whitened&#125;&#125;$。2. **ICA计算**：   - 初始化 $\mathbf&#123;W&#125; = \begin&#123;bmatrix&#125; w_&#123;11&#125; &amp; w_&#123;12&#125; \\ w_&#123;21&#125; &amp; w_&#123;22&#125; \end&#123;bmatrix&#125;$。   - 用FastICA迭代优化 $\mathbf&#123;W&#125;$，最大化 $\mathbf&#123;z&#125;$ 的非高斯性。   - 结果可能是 $\mathbf&#123;W&#125; \approx \begin&#123;bmatrix&#125; 2 &amp; -1 \\ -0.5 &amp; 1.5 \end&#123;bmatrix&#125;$，使得 $\mathbf&#123;z&#125;_1 \approx \mathbf&#123;s&#125;_1$，$\mathbf&#123;z&#125;_2 \approx \mathbf&#123;s&#125;_2$。------#### 6. ICA的应用场景- **信号分离**：从多个麦克风的混合录音中分离每个人的声音。- **图像处理**：从混合图像中提取独立特征（如分离背景和前景）。- **脑电图分析**：从EEG信号中分离独立的脑电波。------#### 7. 总结这张图片介绍了ICA的基本概念和实现：- **概念**：通过线性变换 $\mathbf&#123;z&#125; = \mathbf&#123;W&#125; \cdot \mathbf&#123;x&#125;$ 从混合数据中提取独立成分。- **流程**：先用PCA预处理（降维和去相关），再用ICA分离独立信号。- **数学实现**：中心化 → 白化 → 优化 $\mathbf&#123;W&#125;$（如FastICA）。- **与PCA区别**：PCA关注方差，ICA关注独立性。</code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>transformer学习记录</title>
      <link href="/2025/03/10/transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"/>
      <url>/2025/03/10/transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h2 id="transformer学习记录"><a href="#transformer学习记录" class="headerlink" title="transformer学习记录"></a>transformer学习记录</h2><h3 id="1-注意力机制"><a href="#1-注意力机制" class="headerlink" title="1. 注意力机制"></a>1. 注意力机制</h3><p><strong>重点</strong>：<strong>词嵌入机制</strong>（embeddings）</p><h4 id="1-1-embeddings机制"><a href="#1-1-embeddings机制" class="headerlink" title="1.1 embeddings机制"></a>1.1 embeddings机制</h4><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202503102107397.png" alt="image-20250310210751327"></p><p>对于上述遇到的一些具体歧义的单词时，如Apple，既可以代表水果，也可以代表苹果品牌。传统的embeddings技术如onehot和wordvector很难去进行区分具有歧义的单词，在创建好的词嵌入矩阵中，一个词只能用一个预训练好的向量去表示。</p><h4 id="1-2-Attention机制"><a href="#1-2-Attention机制" class="headerlink" title="1.2 Attention机制"></a>1.2 Attention机制</h4><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202503102114788.png" alt="image-20250310211423751"></p><p>第一句中的Apple代表水果中的苹果，而第二句中代表的是品牌中的苹果。</p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202503102120828.png" alt="image-20250310212005727"></p><p>第一句中的苹果更倾向于水果类，因此我们将苹果的坐标向水果类移动；第二句中的水果倾向于电子产品类，因此向手机的坐标移动。</p><p>Attention的操作就是对句子作一个并行处理。在开始时进行一个初始坐标赋值全部打乱，然后计算句子中的单词相似度来调整坐标，当两个单词意思相近时，他们之间的权重就越大，例如orange和apple。</p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202503102135998.png" alt="image-20250310213518916"></p><p>类似于聚类的感觉，将其向水果类拉近。</p><p><strong>多头注意力机制 Multi-head attention</strong>：假设我们现在初始化三个embedding矩阵，可以很明显看到第一个矩阵的初始化效果更好，apple这个词在手机和水果分类的中间，apple离两种歧义分的越开，那么他的效果就更好，运行效率更好，多头注意力机制就是找出这样一个最佳的原始词嵌入矩阵，此时我们遇到的新问题就是取多少个矩阵更合适。多头注意力机制并不在纵向上叠加来拉升注意力机制的维度，而在横向添加来进行并行处理。<strong>通过拆分特征维度到多个子空间（多头），实现特征学习的多样性和并行化，最后通过线性层进行融合。</strong></p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202503102137786.png" alt="image-20250310213752725"></p><p>我们可以通过线性变换来进行变换</p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202503102144895.png" alt="image-20250310214406820"></p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202503102144528.png" alt="image-20250310214453461"></p><p><strong>输入</strong> → <strong>线性变换生成Q、K、V</strong> → <strong>拆分多头</strong> → <strong>缩放点积注意力</strong> → <strong>拼接多头结果</strong> → <strong>线性变换输出</strong></p><p>左边是多头注意力机制的架构图，右边是上述的过程。我们首先初始化三个词嵌入矩阵，然后通过三个权重矩阵W^Q,W^K,W^V对输入进来的三个词嵌入矩阵进行线性变换得到Q,K,V</p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202503110222377.png" alt="image-20250311022221286"></p><p><strong>Q、K、V是输入向量经过线性变换后的结果</strong>，而非直接等于词嵌入或权重矩阵。经过这种变换我们可得Q,K,V，然后输入进线性层，Q和K相乘进行一个线性变换，变换完成后输入缩放点积注意力层，在这层中，使用V矩阵对其进行一个打分<img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202503110228473.png" alt="image-20250311022854441">，如右图中间所示，初始值为1，经过一次线性变换后得到我们想要的结果，然后再使用softmax函数进行一次缩放以防梯度消失，dk为K的维度（如Apple(0, 1, 2, 3)，那么dk就为4），随后得到一个注意力权重分布，最后用注意力权重对V加权求和，得到当前头的输出<img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202503110232530.png" alt="image-20250311023206492">，并将其输出到连接层。“注意力权重”是通过Q和K的点积计算得到的（即 <img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202503110224957.png" alt="image-20250311022414926">)。</p><h4 id="1-3-结构详解"><a href="#1-3-结构详解" class="headerlink" title="1.3 结构详解"></a>1.3 结构详解</h4><p><strong>宏观理解网络架构</strong></p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202503121539309.png" alt="image-20250312153936210"></p><p>transformer网络分为编码器和解码器，以句子翻译为例，当句子输入到网络中，首先经过多层编码器，每一层的编码器输入都是上一层的输出直到最后一层的输出将输入到每一层的解码器中，然后依次类推，最后输出翻译结果。</p><p>将每一层的编码器和解码器结构拆开进一步分析的话，如下图所示。</p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202503121543961.jpeg" alt="img"></p><p>每一个编码器 在结构上都是一样的，但它们的权重参数是不同的。每一个编码器里面，可以分为 2 层</p><ol><li>Self-Attention Layer</li><li>Feed Forward Neural Network（前馈神经网络，缩写为 FFNN）</li></ol><p>同理，解码器也具有这两层，但是这两层中间还插入了一个 Encoder-Decoder Attention 层，这个层能帮助解码器聚焦于输入句子的相关部分（类似于 seq2seq 模型 中的 Attention）。</p><p><strong>具体结构</strong></p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202503121808683.png" alt="image-20250312180847614"></p><p>编码器部分N个block堆叠组成（自注意+前馈网络），每个block又分为两层：自注意力+残差连接（参考CNN架构）+归一化层&#x2F;前馈神经网络+残差连接+归一化层。每一个block依次向上传递直到最后第N层，将第N层的block输出到解码器的每一层中（参考上述encoder和decoder图）。</p><p>解码器部分中Output Embedding输入是我们已知的句子翻译结果再输入（以句子翻译为例就是假设Iuputs输入为我是学生，那么最后的输出结果为I am a student，那么这里的Output Embedding就是翻译结果），每个block层又分为如上图右边的结构：掩码自注意力层+残差+归一化&#x2F;自注意力层+残差+归一化&#x2F;前馈神经网络+残差+归一化层。这里需要特别注意的是，中间的自注意力层的输入来自编码器最后一层的输出以及来自掩码注意力层的输出共同组合。</p><p>最后解码器的输出会经过一个线性层和softmax函数激活后输出我们词表中所有词的概率，选择概率最大的一个作为我们最后的结果输出。</p><p><strong>词嵌入（Word Embedding）</strong></p><p>我们首先将每个输入单词通过词嵌入算法转换为词向量。在transformer中，通常转换为256 或者 512 维。</p><p>所有的编码器都有一个相同的特点，即它们接收一个向量列表，列表中的每个向量大小特定维度，在Transformer中是512维。词嵌入过程只发生在最底层的编码器中。在底层（最开始）编码器中它就是词向量，但是在其他编码器中，它就是下一层编码器的输出（也是一个向量列表）。</p><p>向量列表大小是我们可以设置的超参数：一般是我们训练集中最长句子的长度。</p><p><strong>位置编码（Positional Encoding）</strong></p><p>首先我们回顾RNN，RNN隐藏层的每一次计算都会包含上一时刻的信息，而transformer并没有这个结构，因此我们需要用到位置编码来告诉网络当前这个单词在整个句子中的位置，他通过和input embedding相加来得到一个同样为512维度的向量作为编码器的输入。</p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202503122124562.png" alt="image-20250312212410515"></p><p>pos指当前词在整个句子中的位置，范围是0到整个句子的长度（左闭右开）。i指位置编码的维度，范围是0到2&#x2F;dmodel（左闭右开，通常为512维）。</p><p>当维数为偶数时输入到sin函数，维数为奇数时输入到cos函数。</p><p><strong>编码过程（宏观）</strong></p><p>Transformer的一个核心特性，在这里输入序列中每个位置的单词都有自己独特的路径流入编码器。在自注意力层中，这些路径之间存在依赖关系。而前馈(feed-forward)层没有这些依赖关系。因此在前馈(feed-forward)层时可以并行执行各种路径。</p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202503122203069.png" alt="image-20250312220353012"></p><p>以下图为例，其中的“it”这个句子是指什么呢？它指的是street还是这个animal呢？这对于人类来说是一个简单的问题，但是对于算法则不是。当模型处理这个单词“it”的时候，自注意力机制会允许“it”与“animal”建立联系。随着模型处理输入序列的每个单词，自注意力会关注整个输入序列的所有单词，帮助模型对本单词更好地进行编码。</p><p>如果你熟悉RNN（循环神经网络），回忆一下它是如何维持隐藏层的。RNN会将它已经处理过的前面的所有单词&#x2F;向量的表示与它正在处理的当前单词&#x2F;向量结合起来。而自注意力机制会将所有相关单词的理解融入到我们正在处理的单词中。</p><p>当我们在编码器#5（栈中最上层编码器）中编码“it”这个单词的时，注意力机制的部分会去关注“The Animal”，将它的表示的一部分编入“it”的编码中。</p><p>注意力机制的精髓就体现在这些深浅不一的橙色背景色中。</p><p>为了让大家更好地理解注意力的作用，再举个计算机视觉中的例子，假设我们要对一张图片进行分类，注意力的作用就在于帮助我们指出图片中的哪些区域是值得关注的。可以看到经过自注意力机制已经很明显地将兔子的轮廓及其标志性的地方用热力图框选出来了。</p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202503122205935.png" alt="image-20250312220517870"></p><p><strong>编码过程（微观）</strong></p><p>第一步，生成 Q、K、V，辅助计算注意力机制</p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202503122218105.png" alt="image-20250312221807056"></p><p>通过将X1，X2向量分别与权重矩阵W^Q^, W^K^, W^V^相乘，为每个单词都创建一个查询向量Q，一个键向量K，值向量V。</p><p>第二步，计算当前单词的Q与候选单词的K的点积，得到注意力分数。假设我们在为这个例子中的第一个词“Thinking”计算自注意力向量，我们需要拿输入句子（Thinking Machines）中的每个单词对“Thinking”打分。这些分数决定了在编码单词“Thinking”的过程中有多重视句子的其它部分。这些分数是通过打分单词（所有输入句子的单词）的键向量与“Thinking”的查询向量相点积来计算的。所以如果我们是处理位置最靠前的词的自注意力的话，第一个分数是q1和k1的点积，第二个分数是q1和k2的点积。</p><p>第三步，将上一步结果除以维度的平方根（使梯度更加稳定，这里假设转换为词向量后的维度为64）</p><p>第四步，将上一步结果通过softmax函数转换（softmax的作用是使所有单词的分数归一化，得到的分数都是正值且和为1。这个softmax分数决定了每个单词对编码当下位置（“Thinking”）的贡献。显然，已经在这个位置上的单词将获得最高的softmax分数，但有时关注另一个与当前单词相关的单词也会有帮助。）</p><p>第五步，将候选单词的每个值向量V乘以softmax后的分数。</p><p>第六步，对加权后的值向量V求和，即可得到自注意力层在该位置上的输出（在我们的例子中是对于第一个单词）。得到的输出就可以输入到前馈神经网络中。</p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202503130026008.png" alt="image-20250313002553906"></p><p><strong>核心公式</strong>：<img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202503130046166.png" alt="image-20250313004615124"></p><p>通过矩阵运算实现：</p><p>第一步是计算查询矩阵、键矩阵和值矩阵。为此，我们将将输入句子的词嵌入装进矩阵X中，将其乘以我们训练的权重矩阵(WQ ， WK， WV)。</p><p>X矩阵中的每一行对应于输入句子中的一个单词。我们再次看到词嵌入向量 (512，或图中的4个格子)和q&#x2F;k&#x2F;v向量(64，或图中的3个格子)的大小差异。</p><p>最后，由于我们处理的是矩阵，我们可以用一个公式来计算自注意力层的输出。</p><p>到这里，通过矩阵运算实现自注意力的计算就完成了。这时，我们对于输入句子向量，得到一组Z向量，输入到后续的前馈神经网络中。</p><p>但实际情况下，在Transformer中，采用的方式就是多头注意力机制，这里的多头可以理解为不同的QKV（就是在纵向叠加多个注意力层来进行同一个输入并计算，但是权重矩阵的初始化是不同的，因此得到的QKV矩阵也不同，注意力分数也不同）。</p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202503130110539.png" alt="image-20250313011014487"></p><p><strong>多头注意力机制</strong></p><p>直观来理解其实就是对于同一个词向量X输入到多个注意力层中进行多次不同的运算，虽然共享同一个输入，但是由于矩阵初始化值的不同会导致其获得不同的QKV矩阵，注意力分数矩阵Z的结果也因此不同。假设这里我们设置了8个注意力头，得到了8个不同的注意力头Z<del>0</del>，…，Z<del>7</del>，我们直接将这8个矩阵进行拼接成一个矩阵Z然后输入到前馈神经网络中。</p><p>原论文中说到进行Multi-head Attention的原因是将模型分为多个头，形成多个子空间，可以让模型去关注不同方面的信息，最后再将各个方面的信息综合起来。其实直观上也可以想到，如果自己设计这样的一个模型，必然也不会只做一次attention，多次attention综合的结果至少能够起到增强模型的作用，也可以类比CNN中同时使用多个卷积核的作用，直观上讲，多头的注意力有助于网络捕捉到更丰富的特征&#x2F;信息。</p><p>通过增加一种叫做“多头”注意力（“multi-headed” attention）的机制，论文进一步完善了自注意力层，并在两方面提高了注意力层的性能：</p><p>1.它扩展了模型专注于不同位置的能力。在上面的例子中，虽然每个编码都在z1中有或多或少的体现，但是它可能被实际的单词本身所支配。如果我们翻译一个句子，比如“The animal didn’t cross the street because it was too tired”，我们会想知道“it”指的是哪个词，这时模型的“多头”注意机制会起到作用。</p><p>2.它给出了注意力层的多个“表示子空间”（representation subspaces）。接下来我们将看到，对于“多头”注意机制，我们有多个查询&#x2F;键&#x2F;值权重矩阵集(Transformer使用八个注意力头，因此我们对于每个编码器&#x2F;解码器有八个矩阵集合)。这些集合中的每一个都是随机初始化的，在训练之后，每个集合都被用来将输入词嵌入(或来自较低编码器&#x2F;解码器的向量)投影到不同的表示子空间中。</p><p>在“多头”注意机制下，我们为每个头保持独立的查询&#x2F;键&#x2F;值权重矩阵，从而产生不同的查询&#x2F;键&#x2F;值矩阵。和之前一样，我们拿X乘以WQ&#x2F;WK&#x2F;WV矩阵来产生查询&#x2F;键&#x2F;值矩阵。</p><p>如果我们做与上述相同的自注意力计算，只需八次不同的权重矩阵运算，我们就会得到八个不同的Z矩阵。这就类似于在卷积神经网络中使用多个不同尺寸的filter提取多种特征。</p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202503130112534.png" alt="image-20250313011242480"></p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202503130120601.png" alt="image-20250313012013546"></p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202503130120098.png" alt="image-20250313012034054"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> transformer </tag>
            
            <tag> 词嵌入 </tag>
            
            <tag> embeddings </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python</title>
      <link href="/2025/03/07/python%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"/>
      <url>/2025/03/07/python%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h3 id="关于-init-py"><a href="#关于-init-py" class="headerlink" title="关于__ init __.py"></a>关于__ init __.py</h3><p>假设现在有如下结构</p><p>project-name&#x2F;<br>│<br>├──package<br>│   ├── __ init __.py<br>│   └── a.py<br>│<br>├──b.py</p><p>现在a.py的内容为</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(&quot;hello world!&quot;)</span><br></pre></td></tr></table></figure><p>b.py内容为</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import package</span><br></pre></td></tr></table></figure><p>此时运行b.py不会输出任何内容，而此时我们在__ init __.py中加入如下内容</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(&quot;hello world!&quot;)</span><br></pre></td></tr></table></figure><p>此时我们再次运行b.py，会看到终端窗口输出hello world!</p><p>这说明我们在<code>import package</code>时调用的package文件夹时，默认运行的是__ init __ .py文件。</p><p>当一个目录包含 <code>__init__.py</code> 时，Python 会将其视为一个“包”（Package），而非普通目录。这使得你可以通过 <code>import 包名.模块名</code> 的方式导入模块。</p><p>作用主要分三类：1.包的初始化；2.管理包接口；3.包的信息</p><p>参考：<a href="https://www.bilibili.com/video/BV1QA94YPEMK/?spm_id_from=333.1007.top_right_bar_window_default_collection.content.click&vd_source=da89301766d2e999eee1ad133fd0a648">https://www.bilibili.com/video/BV1QA94YPEMK/?spm_id_from=333.1007.top_right_bar_window_default_collection.content.click&amp;vd_source=da89301766d2e999eee1ad133fd0a648</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>模型剪枝</title>
      <link href="/2025/03/07/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E5%89%AA%E6%9E%9D/"/>
      <url>/2025/03/07/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E5%89%AA%E6%9E%9D/</url>
      
        <content type="html"><![CDATA[<table><thead><tr><th align="left"><strong>符号</strong></th><th align="left"><strong>定义</strong></th><th align="center"><strong>在 PruneFL 中的角色</strong></th></tr></thead><tbody><tr><td align="left"><strong>⊙</strong></td><td align="left">两个向量的逐元素乘积</td><td align="center">用于掩码操作，例如将参数向量与掩码向量相乘，保留重要参数，剪枝不重要参数。</td></tr><tr><td align="left"><strong>n, N</strong></td><td align="left">客户端索引（单个设备）、客户端总数</td><td align="center">表示参与联邦学习的边缘设备，N 是设备总数，用于分布式训练和参数聚合。</td></tr><tr><td align="left"><strong>k, K</strong></td><td align="left">迭代索引（通信轮次）、总迭代次数</td><td align="center">表示联邦学习的通信轮次，k 是当前轮次，K 是训练总轮次。</td></tr><tr><td align="left"><strong>I</strong></td><td align="left">本地迭代次数（每个客户端的 SGD 步骤数）</td><td align="center">客户端在本地数据上进行的参数更新次数，影响本地计算开销和模型收敛速度。</td></tr><tr><td align="left"><strong>pₙ</strong></td><td align="left">客户端 n 的聚合权重</td><td align="center">用于服务器聚合客户端参数时的加权平均，例如根据客户端数据量分配权重。</td></tr><tr><td align="left"><strong>m(k)</strong></td><td align="left">第 k 次迭代中的权重掩码（所有客户端共享）</td><td align="center">动态剪枝的关键，掩码中 0 表示参数被剪枝，1 表示保留。通过自适应策略更新，减少模型大小和通信开销。</td></tr><tr><td align="left"><strong>wₙ(k)</strong></td><td align="left">客户端 n 在第 k 次迭代时的本地参数</td><td align="center">客户端本地训练后的模型参数，上传至服务器进行聚合。</td></tr><tr><td align="left"><strong>w(k)</strong></td><td align="left">全局参数（加权平均）</td><td align="center">服务器聚合后的全局模型参数，下发至各客户端进行下一轮训练。</td></tr><tr><td align="left"><strong>w’ₙ(k), w’(k)</strong></td><td align="left">剪枝后的参数</td><td align="center">应用掩码后的稀疏参数，仅保留重要参数，用于减少计算和通信负载。</td></tr><tr><td align="left"><strong>gₙ(w)</strong></td><td align="left">客户端 n 在参数 w 处的随机梯度（w为模型参数向量）</td><td align="center">本地 SGD 计算的梯度，用于更新本地参数。在剪枝中可能用于评估参数重要性。</td></tr><tr><td align="left"><strong>∇Fₙ(w)</strong></td><td align="left">客户端 n 在参数 w 处的期望梯度</td><td align="center">理论上的平均梯度，与随机梯度相对，用于收敛性分析。</td></tr><tr><td align="left"><strong>∇F(w)</strong></td><td align="left">全局期望梯度</td><td align="center">全局模型的梯度方向，指导参数更新。在剪枝中可能用于优化掩码选择策略。</td></tr></tbody></table><p>模型剪枝也叫模型稀疏化，即将部分对网络贡献较小的特征剔除或者置0，稀疏化可以分为两类：权重稀疏化（将网络中某些权重值设为0或删除，通过L1正则化或剪枝技术）和结构稀疏化（神经元剪枝）。模型中可以被剪枝（稀疏）的对象通常为权重、神经元（激活函数）、梯度。可以认为模型稀疏或剪枝的方法为置0和删除，个人认为稀疏对应置0，剪枝对应删除。</p><h4 id="1-权重稀疏"><a href="#1-权重稀疏" class="headerlink" title="1. 权重稀疏"></a>1. 权重稀疏</h4><p>通常认为权重的绝对值大小是一种重要的衡量指标，权重数值越大代表对网络的输出贡献也越大，反之则不重要，删去对网络的影响也较小。</p><p>通常通过L1、L2正则化来迫使一些权重趋向于0，将一些低于阈值的权重参数删除，反复迭代这个删除过程知道模型的规模和精度达到预期。因为即使移除绝对值接近0的权重也会导致推理精度的损失，所以通常在剪枝后需要再训练。剪枝常做的三段式工作：训练、剪枝、微调。</p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/20250307213104479.png"></p><h4 id="2-神经元（激活函数）稀疏"><a href="#2-神经元（激活函数）稀疏" class="headerlink" title="2. 神经元（激活函数）稀疏"></a>2. 神经元（激活函数）稀疏</h4><p>首先这里给出现代网络常用激活函数ReLU函数的定义：<code>ReLU(x)=max(0,x)</code>，负值的输入都会被置为0，这种方法也可以认为是通过激活函数的方法给网络进行稀疏化。同样池化操作也可以产生类似形式，受此现象启发，我们发现CNN中许多的神经元都是低激活状态的，因此我们认为<strong>零神经元</strong>可能是冗余的，可以在不影响整体网络精度的情况下移除，即对网络中的神经元进行剪枝，做移除神经元操作。</p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/20250307212857550.png"></p><h4 id="3-梯度稀疏"><a href="#3-梯度稀疏" class="headerlink" title="3. 梯度稀疏"></a>3. 梯度稀疏</h4><p>通常采用选择固定比例的正负梯度更新或预设阈值。</p><p><strong>深度梯度压缩</strong>：在梯度稀疏化基础上采用动量修正、本地梯度剪裁、动量因子遮蔽和 warm-up训练 4 种方法来做梯度压缩，从而减少分布式训练中的通信带宽。</p><p>神经网络的权重、激活和梯度稀疏性总结：</p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/20250307213200481.png"></p><h4 id="4-结构化稀疏"><a href="#4-结构化稀疏" class="headerlink" title="4. 结构化稀疏"></a>4. 结构化稀疏</h4><p>分为Channel&#x2F;Filter 剪枝或shape-wise 剪枝，即对网络的层数（滤波器）或网络的大小进行剪裁。channel 剪枝的工作是最多的，channel 剪枝和 filter 剪枝其实意义是一样的，一个过滤器移除了，对应输出 feature map 的一个通道自然也被移除，反之一样。</p><p><strong>阶段式剪枝</strong>：可以弥补滤波器级剪枝的不足，如下图所示。</p><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202503072144121.jpg" alt="v2-9666cdccffe0dd988c1796049073d4eb_1440w"></p><p>图中N1、N2等代表的是每一层实际保留的通道数，如a)原始模型有64个通道，b)中删减为了N1个。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 论文 </tag>
            
            <tag> 联邦学习 </tag>
            
            <tag> 模型剪枝 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo主题美化设置</title>
      <link href="/2025/03/02/hexo%E4%B8%BB%E9%A2%98%E7%BE%8E%E5%8C%96%E8%AE%BE%E7%BD%AE/"/>
      <url>/2025/03/02/hexo%E4%B8%BB%E9%A2%98%E7%BE%8E%E5%8C%96%E8%AE%BE%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<h2 id="hexo主题美化设置"><a href="#hexo主题美化设置" class="headerlink" title="hexo主题美化设置"></a>hexo主题美化设置</h2><p>hexo下的相关配置分为主站配置和对应的主题配置文件。以本项目为例：项目配置文件在主路径&#x2F;blog下的_config.yml，而和主题相关配置的文件在&#x2F;blog&#x2F;themes&#x2F;hexo-theme-matery下的_config.yml中。</p><p>简单来说，我们可以认为当我们需要改变和当前主题的有关配置时（如美化，更换背景、主题的文字和标签分类），我们可以到主题的配置文件中去寻找。而当我们需要去更改一些全局配置时（如标签页的文字，网站的语言和主标题），我们应当到主站的配置文件中去寻找。</p>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>在mac上使用github + hexo 建立个人博客</title>
      <link href="/2025/03/02/github%E5%8D%9A%E5%AE%A2%E5%BB%BA%E7%AB%99/"/>
      <url>/2025/03/02/github%E5%8D%9A%E5%AE%A2%E5%BB%BA%E7%AB%99/</url>
      
        <content type="html"><![CDATA[<h1 id="mac上使用github-hexo建立个人博客"><a href="#mac上使用github-hexo建立个人博客" class="headerlink" title="mac上使用github + hexo建立个人博客"></a>mac上使用github + hexo建立个人博客</h1><h2 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1. 准备工作"></a>1. 准备工作</h2><p>需要一个github账号和科学上网环境，所有操作推荐在<code>sudo su</code>的root环境下进行</p><h2 id="2-github上的相关配置"><a href="#2-github上的相关配置" class="headerlink" title="2. github上的相关配置"></a>2. github上的相关配置</h2><h3 id="2-1-建立博客的存储仓库"><a href="#2-1-建立博客的存储仓库" class="headerlink" title="2.1 建立博客的存储仓库"></a>2.1 建立博客的存储仓库</h3><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202503021627761.png"></p><p>在github上建立一个<strong>公开仓库</strong>（一定要为<strong>公开</strong>），在<strong>repository name</strong>处填入username.github.io（username为前面owner下的id，注意一定要和前面一样，可以不区分大小写），随后建立仓库</p><p>补充：推荐创建两个仓库，一个公有，一个私有。部署网页的公开仓库和本地项目文件不一样，上传到该仓库的只有静态网页文件，不包含本地的整个项目文件，更换设备进行维护时会很不方便。</p><h3 id="2-2-SSH-key配置"><a href="#2-2-SSH-key配置" class="headerlink" title="2.2 SSH key配置"></a>2.2 SSH key配置</h3><p><img src="https://cdn.jsdelivr.net/gh/FeiZao825/picture-bed@main/img/202503021652121.png"></p><p>在设置中的<strong>SSH and GPG keys</strong>选项下选择<strong>New SSH key</strong>。此处需要注意的是mac系统在非root用户权限下ssh密钥的相关配置是独立的，推荐使用sudo su进入临时root权限后再新建一对ssh密钥，将该密钥下的id_rsa.pub中的ssh key填入此界面，否则在之后推送到仓库时可能会出现public key的问题。</p><h2 id="3-安装node-js"><a href="#3-安装node-js" class="headerlink" title="3. 安装node.js"></a>3. 安装node.js</h2><ol><li><p>官网选择macos和arm64或者打开终端输入：<code>curl https://raw.github.com/creationix/nvm/master/install.sh | sh</code></p></li><li><p>终端输入node -v &#x2F; npm -v，显示版本号即安装成功</p></li></ol><h2 id="4-安装hexo"><a href="#4-安装hexo" class="headerlink" title="4.安装hexo"></a>4.安装hexo</h2><p>终端输入：<code>sudo npm install hexo-cli -g</code></p><h2 id="5-使用hexo创建博客"><a href="#5-使用hexo创建博客" class="headerlink" title="5.使用hexo创建博客"></a>5.使用hexo创建博客</h2><ol><li>使用hexo初始化一个叫blog的文件夹并切换到blog文件夹：<code>hexo init blog &amp;&amp; cd blog</code></li><li>安装hexo-deployer-git自动部署发布工具：<code>npm install hexo-deployer-git --save</code></li><li>进入<em>\Users\apple\blog</em>文件夹，找到*_config.yml*文件，将文件底部的：</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: </span><br></pre></td></tr></table></figure><p>替换成：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">type: git</span><br><span class="line">  repo: git@github.com:FeiZao825/FeiZao825.github.io.git</span><br><span class="line">  branch: main</span><br></pre></td></tr></table></figure><h2 id="5-预览并发布博客"><a href="#5-预览并发布博客" class="headerlink" title="5.预览并发布博客"></a>5.预览并发布博客</h2><ol><li><p>生成网页静态文件到默认的<strong>public</strong>文件夹中：<code>hexo g</code></p></li><li><p>运行在本地并预览网页：<code>hexo s</code>，可以在”<a href="https://localhost:4000"进行预览">https://localhost:4000&quot;进行预览</a></p></li><li><p>部署到github上实现远程访问：<code>hexo d</code>，运行成功后可以看到仓库中多了静态网页的项目文件并且可以通过“feizao825.github.io”来访问</p><p>注意：<code>hexo d</code>上传的代码只有静态网页文件，和本地<code>hexo init</code>的文件不一样，推荐再创建一个保存本地项目的私有仓库</p></li><li><p>清空一下缓存，有时候博客页面显示不正常也可以试试这个命令行：<code>hexo clean</code></p></li></ol><h2 id="6-更换主题"><a href="#6-更换主题" class="headerlink" title="6.更换主题"></a>6.更换主题</h2><p><code>git clone  项目地址</code></p><p>运行上述命令后，可在&#x2F;themes路径下看到主题的文件夹，此处以<strong>hexo-theme-matery</strong>主题为例。</p><p>在项目主路径&#x2F;blog下的_config.yml中配置</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># theme: landscape  默认主题</span></span><br><span class="line">theme: butterfly</span><br><span class="line"></span><br><span class="line">title: 肥皂的博客</span><br><span class="line">author: FeiZao</span><br><span class="line">language: zh<span class="literal">-CN</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> github </tag>
            
            <tag> 博客 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>电池论文记录</title>
      <link href="/2025/03/02/%E7%94%B5%E6%B1%A0%E8%AE%BA%E6%96%87/"/>
      <url>/2025/03/02/%E7%94%B5%E6%B1%A0%E8%AE%BA%E6%96%87/</url>
      
        <content type="html"><![CDATA[<h2 id="实现过程"><a href="#实现过程" class="headerlink" title="实现过程"></a>实现过程</h2><p>**1.**电池采集原始数据（充放电电压，容量，IC曲线等）做特征工程，提取30个特征点</p><p>**2.**根据特征工程数据集通过同质或异质划分数据集，先对数据集添加噪声，随后将数据分发到各个客户端，客户端对数据进行重采样后进行本地训练。</p><ul><li><p>为了解决数据稀缺性问题，研究采用了<strong>数据增强</strong>技术。具体方法是对每个电池类别的数据进行重采样（Bootstrap），通过有放回采样的方式，增加每个类别的数据量，最终将每个类别的数据扩展到200条记录。</p></li><li><p><strong>高斯噪声添加</strong>：为了进一步保护隐私，并验证模型对噪声的鲁棒性，在增强后的数据上添加了随机高斯噪声，噪声强度由噪声-信号比（NSR）控制，范围从1%到10%。这种噪声的加入是为了评估模型在不同隐私预算下的表现。</p></li><li><p>数据集被分为训练集和测试集，分别按照80%和20%的比例进行划分。此外，在训练集中再进行40%的二次划分，用于交叉验证。</p></li><li><p>数据的划分采用分层抽样的方式，确保在每个客户端的数据样本分布中，各个电池类别都有足够的代表性。</p></li></ul><p><strong>同质 (Homogeneous)</strong></p><ul><li><strong>定义</strong>：同质集成指的是集成模型中的所有基学习器都是同一种类型的模型。在随机森林中，所有基学习器通常都是<strong>决策树</strong>，因此随机森林本身是一个<strong>同质集成模型</strong>。</li><li><strong>特点</strong>：虽然所有基学习器都是决策树，但通过对数据集进行不同的采样（即通过引入随机性，如数据采样和特征选择），每棵决策树的结构和预测结果可能会有所不同。这种通过随机性引入的差异使得同质集成的模型在一定程度上具有多样性。</li></ul><p><strong>例子</strong>：</p><ul><li>随机森林的每个基学习器都是决策树，虽然它们的模型形式相同，但由于每棵树基于不同的数据子集和特征子集构建，因此形成了同质集成。</li></ul><h5 id="异质-Heterogeneous"><a href="#异质-Heterogeneous" class="headerlink" title="异质 (Heterogeneous)"></a>异质 (Heterogeneous)</h5><ul><li><strong>定义</strong>：异质集成是指集成模型中的基学习器由不同类型的模型组成。在这种情况下，集成模型会结合多种不同类型的算法来构建。例如，你可以结合决策树、支持向量机（SVM）、神经网络、k近邻算法等模型。</li><li><strong>特点</strong>：由于不同类型的模型通常具有不同的假设和表现，异质集成模型能够通过结合不同的模型优势，提高整体的预测性能和稳健性。这种方法通常被称为“<strong>堆叠（stacking）</strong>”或“集成学习”的一种复杂形式。</li></ul><p><strong>例子</strong>：</p><ul><li>使用决策树、逻辑回归和支持向量机来创建一个集成模型。不同模型的预测结果可以通过投票、平均或堆叠来聚合。</li></ul><h4 id="区别总结："><a href="#区别总结：" class="headerlink" title="区别总结："></a>区别总结：</h4><ul><li><strong>同质集成</strong>：所有基学习器都是相同类型的模型（如随机森林中的所有树都是决策树）。</li><li><strong>异质集成</strong>：基学习器由不同类型的模型组成（如组合决策树、SVM、神经网络等）。</li></ul><p>随机森林是一种典型的<strong>同质集成模型</strong>，通过引入随机性（如随机选择特征和样本）来增加模型的多样性，从而提升整体表现。</p><p><strong>3.<strong>使用WDV投票机制，将每个客户端中的结果进行聚合，参与者上传给中心服务器的并不是随机森林模型的原始参数或训练数据，而是每个参与者在本地随机森林模型上</strong>预测的结果</strong>。随机森林的聚合通常是通过投票，平均法或bagging等方法对每个随机森林的最后预测结果进行聚合，以投票法为例，可以理解为取随机森林结果中概率最大的那个预测结果。</p><p><strong>本地模型训练与预测结果生成</strong></p><p>每个参与者使用本地数据集训练一个随机森林模型。这些随机森林模型基于本地数据进行训练，而不与其他参与者的数据共享。</p><ul><li>随机森林模型由多个决策树组成，参与者使用本地的电池数据进行训练，生成模型。</li><li>训练完毕后，参与者会根据本地模型对测试数据或实时数据做出预测，生成预测结果。</li></ul><p><strong>上传的内容：预测结果</strong></p><p>在联邦学习框架中，参与者并不上传原始数据或者完整的模型参数，而是上传经过模型计算的<strong>预测结果</strong>。这些预测结果可以是对给定输入数据的分类或回归输出。上传的预测结果通过加密保护，保证在传输过程中不会泄露原始数据。</p><p><strong>服务器聚合过程</strong></p><p>服务器接收到各个参与者的预测结果后，通过<strong>Wasserstein距离投票（WDV）</strong>或其他加权投票策略来对这些结果进行聚合。具体步骤如下：</p><ul><li>每个参与者上传的预测结果（如对电池类型的分类）被聚合时，服务器会根据每个参与者与全局模型之间的Wasserstein距离，计算该参与者的权重。</li><li>这些预测结果会通过加权投票的方式整合为最终的全局预测结果。服务器根据各个参与者的预测分布进行加权，得到最符合全局目标的分类结果。</li></ul><p><strong>聚合的目标</strong></p><p>这种聚合方式的目标是通过每个参与者的本地模型提供的预测结果，来生成一个全局模型或决策结果。与传统的模型参数聚合不同，论文采用的是基于各地预测结果的聚合，这样不仅保护了参与者的数据隐私，还能有效处理不同参与者之间的数据异构性问题。</p><p><strong>差分隐私保护</strong></p><p>为了进一步保障隐私，上传的预测结果会在发送之前经过差分隐私处理。例如，参与者会在预测结果上加入高斯噪声，确保即使服务器接收到这些预测结果，也无法还原回原始数据。这样可以有效防止数据泄露，同时保持较高的模型准确性。</p><p>**4.**加入隐私保护机制，通过高斯噪声实现，噪声主要是在模型上传阶段和数据增强阶段引入的。在本地训练完模型之后就会加入噪声，在上传到服务器前就会执行的步骤</p><p>手工实现adam，在adam中添加噪声实现隐私，可以在每个节点上添加噪声</p>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文 </tag>
            
            <tag> 电池 </tag>
            
            <tag> 特征工程 </tag>
            
            <tag> 随机森林 </tag>
            
            <tag> 数据同质和异质性 </tag>
            
            <tag> 数据处理 </tag>
            
            <tag> 联邦学习 </tag>
            
            <tag> admm </tag>
            
            <tag> 隐私保护 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2025/02/23/hello-world/"/>
      <url>/2025/02/23/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>everything软件正则匹配规则</title>
      <link href="/2025/01/21/everything/"/>
      <url>/2025/01/21/everything/</url>
      
        <content type="html"><![CDATA[<h2 id="正则表达式重命名匹配规则"><a href="#正则表达式重命名匹配规则" class="headerlink" title="正则表达式重命名匹配规则"></a>正则表达式重命名匹配规则</h2><p>以“Steins_Gate S01E02]<a href="81BF513C">x264_flac</a>.mkv”为例，快速批量重命名文件为“Steins_Gate S01E02.mkv”</p><p>(Steins_Gate S01E\d+)[.*](.*).(mkv|sc.ass)提取两个括号中的内容，使用\d提取集数，提取mkv和sc.ass</p><p>\1.\2重命名文件，\1匹配上述第一个括号内的内容，\2匹配上述第二个括号.(mkv|sc.ass)的文件后缀</p>]]></content>
      
      
      <categories>
          
          <category> 软件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件 </tag>
            
            <tag> everything </tag>
            
            <tag> nas </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SSH的相关使用</title>
      <link href="/2024/12/31/ssh/"/>
      <url>/2024/12/31/ssh/</url>
      
        <content type="html"><![CDATA[<h2 id="SSH连接密钥生成"><a href="#SSH连接密钥生成" class="headerlink" title="SSH连接密钥生成"></a>SSH连接密钥生成</h2><p>ssh-keygen -t rsa -b 4096 -C “feizao”</p><p><code>ssh-keygen</code> 是用于生成、管理和转换 SSH（Secure Shell）认证密钥的工具。 <code>ssh-keygen -t -rsa -b 4096 -C &quot;feizao&quot;</code> 的含义如下：</p><ul><li><code>-t rsa</code>：指定密钥类型为 RSA。</li><li><code>-b 4096</code>：设置密钥长度为 4096 位。</li><li><code>-C &quot;feizao&quot;</code>：为密钥添加注释 “feizao”。</li></ul><p>执行此命令后，系统会提示您指定保存密钥的位置&#x2F;root&#x2F;.ssh&#x2F;id_rs（默认情况下为 <code>~/.ssh/id_rsa</code>），以及设置用于保护私钥的密码短语（可以为空，但这会降低安全性）。 生成的密钥对包括一个私钥（仅您自己保存）和一个公钥（可分发给需要验证您身份的服务器）。</p><p>.pub指公钥，其功能是需要把它放到例如github、gitee之类的代码平台上，告诉我们这里有一扇门。我们需要使用本地的私钥来访问这扇门，即公钥-私钥的配对，公钥可以理解为门，私钥可以理解为门的钥匙。</p><h2 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h2><p>git init 本地新建并初始化一个仓库</p><p>使用ssh方式git需要在本地.ssh中同时放入.pub和id_rsa，即公钥和私钥来访问私有仓库</p><p><strong>下载代码</strong></p><p>git clone <a href="mailto:&#x67;&#105;&#x74;&#64;&#103;&#x69;&#116;&#x65;&#x65;&#46;&#x63;&#111;&#x6d;">&#x67;&#105;&#x74;&#64;&#103;&#x69;&#116;&#x65;&#x65;&#46;&#x63;&#111;&#x6d;</a>:feizao1145&#x2F;gans-fed.git 获取远程仓库</p><p>git clone *** –depth 10 通过添加 –depth 10来指定获取最近10次的提交结果</p><p>[</p><p>git submodule update –init –recursive  对于一些很久的分支，要用这个更新一下子模块</p><p>git status 查看状态,确定一下是否有没有track的子模块</p><p>]</p><p><strong>提交代码</strong></p><p>git config –global user.email “<a href="mailto:&#49;&#x33;&#x36;&#56;&#50;&#57;&#53;&#x33;&#53;&#57;&#x40;&#x71;&#113;&#46;&#99;&#x6f;&#109;">&#49;&#x33;&#x36;&#56;&#50;&#57;&#53;&#x33;&#53;&#57;&#x40;&#x71;&#113;&#46;&#99;&#x6f;&#109;</a>“ 提交者邮箱</p><p>git config –global user.name “feizao” 提交者姓名</p><p>通过上述代码可以看到提交者是谁，通过git log查看提交记录</p><p>git checkout -b feizao&#x2F;1221_test 在现有分支（默认分支）上克隆创建一个新的分支，尽量用日期命名，过几天可以换一个新的分支来维护</p><p>TIPS:vscode中代码旁标绿或文件旁边有M(modify)代表该文件有修改</p><p>git add xxx.py 添加更改的文件到缓存区，最好是改一步就提交一次，不要一次性改完再提交</p><p>git commit -m “xxxx” 添加评论</p><p>截止目前的操作都在本地进行，下一步将本地操作上传到远程仓库</p><p>git push –set-upstream origin feizao&#x2F;1221_test 第一次提交创建的分支时需使用该命令来在远程仓库的origin原始仓库新建一个分支</p><p>也可以在vscode中选择当前的分支，在当前分支基础上生成一个新的分支</p><p>git reset –soft HEAD^ 将上次提交的代码回退，撤销本次修改 </p><h2 id="合并分支代码到master"><a href="#合并分支代码到master" class="headerlink" title="合并分支代码到master"></a>合并分支代码到master</h2><p>1.先从分支切换到master</p><p>​git checkout master</p><p>2.拉取远程master分支最新代码：合并前确保master分支本地与远程同步</p><p>​git pull origin master</p><p>3.合并分支（1221_test）到master</p><p>​git merge 1221_test 合并分支到当前所处分支</p><p>4.推送合并后的本地master分支到远程</p><p>​git push origin master</p><p>5.（可选）删除分支，假设该分支的开发任务已完成不需要了</p><p>​删除本地：git branch -d 1221_test</p><p>​删除远程：git push origin –delete 1221_test</p><h2 id="pytorch的一些记录"><a href="#pytorch的一些记录" class="headerlink" title="pytorch的一些记录"></a>pytorch的一些记录</h2><p>loss.backward() 这个函数并不针对具体哪个模型的某个参数，而是对loss这个变量中包含的所有参数进行求导来得到梯度，然后在通过optimizer.step()优化器来进行梯度更新，因此会对loss中所包含的所有参数梯度进行更新</p>]]></content>
      
      
      <categories>
          
          <category> ssh </category>
          
      </categories>
      
      
        <tags>
            
            <tag> github </tag>
            
            <tag> ssh </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DLADMM算法</title>
      <link href="/2024/10/20/dladmm/"/>
      <url>/2024/10/20/dladmm/</url>
      
        <content type="html"><![CDATA[<h2 id="3-DLADMM算法"><a href="#3-DLADMM算法" class="headerlink" title="3  DLADMM算法"></a>3  DLADMM算法</h2><h4 id="3-1"><a href="#3-1" class="headerlink" title="3.1"></a>3.1</h4><p>将原有的深度学习的目标即损失函数转化为一个优化问题</p><p>首先介绍了深度学习网络的基本结构以及反向传播算法，其次定义了优化问题1即损失函数，通过最小化损失函数来接近正确值，同时为了防止过拟合，引入了正则化项，为了确保优化过程中参数更新能够保持每一层之间的关系，引入了约束条件：线性约束与激活函数的约束。</p><p>松弛问题：<a href="https://blog.csdn.net/universsky2015/article/details/135796968">https://blog.csdn.net/universsky2015/article/details/135796968</a> </p><p>深度网络中涉及到非线性和高维的参数，直接求解优化问题较困难，因此引入了松弛定义来简化问题解决问题2，将问题1的约束条件进行了简化，通过松弛，问题的<strong>硬性约束</strong>被转化为<strong>惩罚项</strong>，并且并入了目标函数的一部分。这样，原本必须满足的约束条件现在只需要逐渐逼近就可以了。ADMM本身是一种将复杂优化问题拆分为若干个子问题分别求解的优化方法，松弛项的引入使得每个子问题都更加易于处理，从而提高了整体算法的效率。</p><h4 id="3-2-DLADMM算法"><a href="#3-2-DLADMM算法" class="headerlink" title="3.2 DLADMM算法"></a>3.2 DLADMM算法</h4><p>与传统的反向算法相比，在反向传播计算梯度更新权重和参数之后引入了向前传播再一次进行求梯度更新参数，这一步补充了传统BP算法的不足，特别是对前几层（靠近输入层）的调整更加及时，因为这几层在传统的BP中往往要等到误差传递过来之后才能更新。而向前更新能够让这些层更早地得到反馈信息，从而加快了模型的收敛速度。</p><p>admm的基础思想是将一个复杂的优化问题分为多个子问题。<strong>梯度下降</strong>主要用于<strong>无约束优化问题</strong>，即模型的目标是最小化一个损失函数，但没有特定的约束条件。<strong>ADMM</strong>更适合处理<strong>带有约束的优化问题</strong>，它通过引入拉格朗日乘子，将约束条件转化为可解的子问题。可以将admm理解为并行处理多个问题。惩罚函数的作用是将有约束转换为无约束</p><p>dlADMM的核心在于通过<strong>增广拉格朗日函数</strong>解决深度学习中的优化问题，并结合ADMM的方法将问题分解为多个小问题。增广拉格朗日函数用于处理神经网络的参数优化，并通过加入惩罚项来约束层与层之间的关系</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> dladmm </tag>
            
            <tag> 梯度更新 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
